Exp  1 started at  2018-12-12 20:02:48 +0100
Exp 1's result is {"10.158.0.1"=>{:stderr=>"18/12/12 19:02:53 INFO terasort.TeraSort: starting18/12/12 19:02:54 INFO input.FileInputFormat: Total input files to process : 2318/12/12 19:02:55 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/12 19:02:56 INFO mapreduce.JobSubmitter: number of splits:87418/12/12 19:02:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/12 19:02:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544641167813_000118/12/12 19:02:56 INFO impl.YarnClientImpl: Submitted application application_1544641167813_000118/12/12 19:02:56 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544641167813_0001/18/12/12 19:02:56 INFO mapreduce.Job: Running job: job_1544641167813_000118/12/12 19:03:03 INFO mapreduce.Job: Job job_1544641167813_0001 running in uber mode : false18/12/12 19:03:03 INFO mapreduce.Job:  map 0% reduce 0%18/12/12 19:03:13 INFO mapreduce.Job:  map 1% reduce 0%18/12/12 19:03:15 INFO mapreduce.Job:  map 13% reduce 0%18/12/12 19:03:16 INFO mapreduce.Job:  map 29% reduce 0%18/12/12 19:03:17 INFO mapreduce.Job:  map 31% reduce 0%18/12/12 19:03:23 INFO mapreduce.Job:  map 32% reduce 0%18/12/12 19:03:24 INFO mapreduce.Job:  map 34% reduce 0%18/12/12 19:03:25 INFO mapreduce.Job:  map 40% reduce 0%18/12/12 19:03:26 INFO mapreduce.Job:  map 52% reduce 0%18/12/12 19:03:27 INFO mapreduce.Job:  map 61% reduce 0%18/12/12 19:03:32 INFO mapreduce.Job:  map 62% reduce 0%18/12/12 19:03:33 INFO mapreduce.Job:  map 63% reduce 4%18/12/12 19:03:34 INFO mapreduce.Job:  map 64% reduce 4%18/12/12 19:03:35 INFO mapreduce.Job:  map 71% reduce 4%18/12/12 19:03:36 INFO mapreduce.Job:  map 80% reduce 4%18/12/12 19:03:37 INFO mapreduce.Job:  map 90% reduce 4%18/12/12 19:03:38 INFO mapreduce.Job:  map 92% reduce 4%18/12/12 19:03:39 INFO mapreduce.Job:  map 92% reduce 6%18/12/12 19:03:40 INFO mapreduce.Job:  map 93% reduce 6%18/12/12 19:03:41 INFO mapreduce.Job:  map 94% reduce 6%18/12/12 19:03:42 INFO mapreduce.Job:  map 96% reduce 6%18/12/12 19:03:43 INFO mapreduce.Job:  map 97% reduce 6%18/12/12 19:03:44 INFO mapreduce.Job:  map 97% reduce 7%18/12/12 19:03:45 INFO mapreduce.Job:  map 98% reduce 8%18/12/12 19:03:46 INFO mapreduce.Job:  map 100% reduce 8%18/12/12 19:03:50 INFO mapreduce.Job:  map 100% reduce 9%18/12/12 19:03:51 INFO mapreduce.Job:  map 100% reduce 11%18/12/12 19:03:57 INFO mapreduce.Job:  map 100% reduce 13%18/12/12 19:04:03 INFO mapreduce.Job:  map 100% reduce 15%18/12/12 19:04:08 INFO mapreduce.Job:  map 100% reduce 16%18/12/12 19:04:09 INFO mapreduce.Job:  map 100% reduce 18%18/12/12 19:04:15 INFO mapreduce.Job:  map 100% reduce 20%18/12/12 19:04:20 INFO mapreduce.Job:  map 100% reduce 21%18/12/12 19:04:21 INFO mapreduce.Job:  map 100% reduce 22%18/12/12 19:04:26 INFO mapreduce.Job:  map 100% reduce 23%18/12/12 19:04:27 INFO mapreduce.Job:  map 100% reduce 25%18/12/12 19:04:33 INFO mapreduce.Job:  map 100% reduce 27%18/12/12 19:04:38 INFO mapreduce.Job:  map 100% reduce 28%18/12/12 19:04:39 INFO mapreduce.Job:  map 100% reduce 29%18/12/12 19:04:44 INFO mapreduce.Job:  map 100% reduce 30%18/12/12 19:04:45 INFO mapreduce.Job:  map 100% reduce 31%18/12/12 19:04:50 INFO mapreduce.Job:  map 100% reduce 32%18/12/12 19:04:51 INFO mapreduce.Job:  map 100% reduce 33%18/12/12 19:05:20 INFO mapreduce.Job:  map 100% reduce 34%18/12/12 19:05:26 INFO mapreduce.Job:  map 100% reduce 38%18/12/12 19:05:27 INFO mapreduce.Job:  map 100% reduce 39%18/12/12 19:05:32 INFO mapreduce.Job:  map 100% reduce 47%18/12/12 19:05:38 INFO mapreduce.Job:  map 100% reduce 56%18/12/12 19:05:44 INFO mapreduce.Job:  map 100% reduce 63%18/12/12 19:05:50 INFO mapreduce.Job:  map 100% reduce 67%18/12/12 19:05:56 INFO mapreduce.Job:  map 100% reduce 69%18/12/12 19:06:02 INFO mapreduce.Job:  map 100% reduce 71%18/12/12 19:06:08 INFO mapreduce.Job:  map 100% reduce 73%18/12/12 19:06:14 INFO mapreduce.Job:  map 100% reduce 75%18/12/12 19:06:21 INFO mapreduce.Job:  map 100% reduce 77%18/12/12 19:06:27 INFO mapreduce.Job:  map 100% reduce 79%18/12/12 19:06:33 INFO mapreduce.Job:  map 100% reduce 81%18/12/12 19:06:39 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 19:06:45 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 19:06:51 INFO mapreduce.Job:  map 100% reduce 86%18/12/12 19:06:57 INFO mapreduce.Job:  map 100% reduce 88%18/12/12 19:07:03 INFO mapreduce.Job:  map 100% reduce 90%18/12/12 19:07:09 INFO mapreduce.Job:  map 100% reduce 92%18/12/12 19:07:15 INFO mapreduce.Job:  map 100% reduce 94%18/12/12 19:07:21 INFO mapreduce.Job:  map 100% reduce 95%18/12/12 19:07:27 INFO mapreduce.Job:  map 100% reduce 97%18/12/12 19:07:33 INFO mapreduce.Job:  map 100% reduce 98%18/12/12 19:07:39 INFO mapreduce.Job:  map 100% reduce 100%18/12/12 19:07:48 INFO mapreduce.Job: Job job_1544641167813_0001 completed successfully18/12/12 19:07:48 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=381459223348\n\t\tFILE: Number of bytes written=501165418160\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=875\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=862\n\t\tRack-local map tasks=13\n\t\tTotal time spent by all maps in occupied slots (ms)=7814857\n\t\tTotal time spent by all reduces in occupied slots (ms)=2854146\n\t\tTotal time spent by all map tasks (ms)=7814857\n\t\tTotal time spent by all reduce tasks (ms)=2854146\n\t\tTotal vcore-milliseconds taken by all map tasks=7814857\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2854146\n\t\tTotal megabyte-milliseconds taken by all map tasks=8002413568\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=2922645504\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4817543229\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=411605\n\t\tCPU time spent (ms)=14043830\n\t\tPhysical memory (bytes) snapshot=264059125760\n\t\tVirtual memory (bytes) snapshot=1782955548672\n\t\tTotal committed heap usage (bytes)=174694858752\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/12 19:07:48 INFO terasort.TeraSort: donereal\t4m56.077s\nuser\t0m10.882s\nsys\t0m0.947s", :stdout=>"Spent 189ms computing base-splits.Spent 5ms computing TeraScheduler splits.\nComputing input splits took 195msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1236msSpent 1433ms computing partitions.", :status=>0}}
Exp  1, overall time taken is 4 m 56.077 s
Exp  1 termintated at 2018-12-12 20:07:48 +0100

Exp  2 started at  2018-12-12 20:17:47 +0100
Exp 2's result is {"10.158.0.1"=>{:stderr=>"18/12/12 19:17:51 INFO terasort.TeraSort: starting18/12/12 19:17:52 INFO input.FileInputFormat: Total input files to process : 2318/12/12 19:17:54 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/12 19:17:54 INFO mapreduce.JobSubmitter: number of splits:87418/12/12 19:17:54 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/12 19:17:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544642065257_000118/12/12 19:17:55 INFO impl.YarnClientImpl: Submitted application application_1544642065257_000118/12/12 19:17:55 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544642065257_0001/18/12/12 19:17:55 INFO mapreduce.Job: Running job: job_1544642065257_000118/12/12 19:18:02 INFO mapreduce.Job: Job job_1544642065257_0001 running in uber mode : false18/12/12 19:18:02 INFO mapreduce.Job:  map 0% reduce 0%18/12/12 19:18:14 INFO mapreduce.Job:  map 3% reduce 0%18/12/12 19:18:15 INFO mapreduce.Job:  map 16% reduce 0%18/12/12 19:18:16 INFO mapreduce.Job:  map 30% reduce 0%18/12/12 19:18:24 INFO mapreduce.Job:  map 35% reduce 0%18/12/12 19:18:25 INFO mapreduce.Job:  map 42% reduce 0%18/12/12 19:18:26 INFO mapreduce.Job:  map 51% reduce 0%18/12/12 19:18:27 INFO mapreduce.Job:  map 59% reduce 0%18/12/12 19:18:32 INFO mapreduce.Job:  map 59% reduce 3%18/12/12 19:18:33 INFO mapreduce.Job:  map 59% reduce 4%18/12/12 19:18:34 INFO mapreduce.Job:  map 63% reduce 4%18/12/12 19:18:35 INFO mapreduce.Job:  map 70% reduce 4%18/12/12 19:18:36 INFO mapreduce.Job:  map 80% reduce 4%18/12/12 19:18:37 INFO mapreduce.Job:  map 85% reduce 4%18/12/12 19:18:38 INFO mapreduce.Job:  map 88% reduce 5%18/12/12 19:18:39 INFO mapreduce.Job:  map 88% reduce 6%18/12/12 19:18:40 INFO mapreduce.Job:  map 89% reduce 6%18/12/12 19:18:41 INFO mapreduce.Job:  map 90% reduce 6%18/12/12 19:18:42 INFO mapreduce.Job:  map 91% reduce 6%18/12/12 19:18:43 INFO mapreduce.Job:  map 92% reduce 6%18/12/12 19:18:44 INFO mapreduce.Job:  map 94% reduce 8%18/12/12 19:18:46 INFO mapreduce.Job:  map 95% reduce 8%18/12/12 19:18:48 INFO mapreduce.Job:  map 96% reduce 8%18/12/12 19:18:50 INFO mapreduce.Job:  map 96% reduce 10%18/12/12 19:18:51 INFO mapreduce.Job:  map 96% reduce 11%18/12/12 19:18:56 INFO mapreduce.Job:  map 96% reduce 13%18/12/12 19:19:02 INFO mapreduce.Job:  map 96% reduce 15%18/12/12 19:19:03 INFO mapreduce.Job:  map 96% reduce 16%18/12/12 19:19:08 INFO mapreduce.Job:  map 96% reduce 18%18/12/12 19:19:14 INFO mapreduce.Job:  map 96% reduce 21%18/12/12 19:19:20 INFO mapreduce.Job:  map 96% reduce 23%18/12/12 19:19:26 INFO mapreduce.Job:  map 96% reduce 26%18/12/12 19:19:32 INFO mapreduce.Job:  map 96% reduce 28%18/12/12 19:19:38 INFO mapreduce.Job:  map 96% reduce 31%18/12/12 19:19:44 INFO mapreduce.Job:  map 96% reduce 32%18/12/12 19:22:56 INFO mapreduce.Job:  map 97% reduce 32%18/12/12 19:28:35 INFO mapreduce.Job:  map 98% reduce 32%18/12/12 19:33:48 INFO mapreduce.Job:  map 99% reduce 32%18/12/12 19:34:59 INFO mapreduce.Job:  map 100% reduce 32%18/12/12 19:35:03 INFO mapreduce.Job:  map 100% reduce 40%18/12/12 19:35:09 INFO mapreduce.Job:  map 100% reduce 64%18/12/12 19:35:15 INFO mapreduce.Job:  map 100% reduce 68%18/12/12 19:35:21 INFO mapreduce.Job:  map 100% reduce 70%18/12/12 19:35:27 INFO mapreduce.Job:  map 100% reduce 72%18/12/12 19:35:33 INFO mapreduce.Job:  map 100% reduce 74%18/12/12 19:35:39 INFO mapreduce.Job:  map 100% reduce 76%18/12/12 19:35:45 INFO mapreduce.Job:  map 100% reduce 78%18/12/12 19:35:51 INFO mapreduce.Job:  map 100% reduce 80%18/12/12 19:35:57 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 19:36:03 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 19:36:09 INFO mapreduce.Job:  map 100% reduce 86%18/12/12 19:36:15 INFO mapreduce.Job:  map 100% reduce 88%18/12/12 19:36:21 INFO mapreduce.Job:  map 100% reduce 90%18/12/12 19:36:27 INFO mapreduce.Job:  map 100% reduce 92%18/12/12 19:36:33 INFO mapreduce.Job:  map 100% reduce 94%18/12/12 19:36:39 INFO mapreduce.Job:  map 100% reduce 96%18/12/12 19:36:44 INFO mapreduce.Job:  map 100% reduce 97%18/12/12 19:36:45 INFO mapreduce.Job:  map 100% reduce 98%18/12/12 19:36:58 INFO mapreduce.Job:  map 100% reduce 99%18/12/12 19:37:03 INFO mapreduce.Job:  map 100% reduce 100%18/12/12 19:37:10 INFO mapreduce.Job: Job job_1544642065257_0001 completed successfully18/12/12 19:37:10 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=386767829508\n\t\tFILE: Number of bytes written=506474024320\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=12\n\t\tLaunched map tasks=885\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=846\n\t\tRack-local map tasks=39\n\t\tTotal time spent by all maps in occupied slots (ms)=52496070\n\t\tTotal time spent by all reduces in occupied slots (ms)=12270816\n\t\tTotal time spent by all map tasks (ms)=52496070\n\t\tTotal time spent by all reduce tasks (ms)=12270816\n\t\tTotal vcore-milliseconds taken by all map tasks=52496070\n\t\tTotal vcore-milliseconds taken by all reduce tasks=12270816\n\t\tTotal megabyte-milliseconds taken by all map tasks=53755975680\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=12565315584\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4868587519\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=359087\n\t\tCPU time spent (ms)=13553010\n\t\tPhysical memory (bytes) snapshot=264299331584\n\t\tVirtual memory (bytes) snapshot=1783076085760\n\t\tTotal committed heap usage (bytes)=174462599168\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/12 19:37:10 INFO terasort.TeraSort: donereal\t19m19.140s\nuser\t0m13.400s\nsys\t0m1.497s", :stdout=>"Spent 194ms computing base-splits.Spent 6ms computing TeraScheduler splits.Computing input splits took 200msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1183msSpent 1386ms computing partitions.", :status=>0}}
Exp  2, overall time taken is 19 m 19.140 s
Exp  2 termintated at 2018-12-12 20:37:10 +0100

Exp  3 started at  2018-12-12 20:47:09 +0100
Exp 3's result is {"10.158.0.1"=>{:stderr=>"18/12/12 19:47:14 INFO terasort.TeraSort: starting18/12/12 19:47:15 INFO input.FileInputFormat: Total input files to process : 2318/12/12 19:47:15 ERROR terasort.TeraSort: Cannot create file/output/_partition.lst. Name node is in safe mode.\nThe reported blocks 851 needs additional 37 blocks to reach the threshold 0.9990 of total blocks 889.\nThe number of live datanodes 22 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached. NamenodeHostName:node1\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1407)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1395)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2278)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2223)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:728)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:413)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.24:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.5:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.2:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.3:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.22:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073746134_5310 after checking nodes = [DatanodeInfoWithStorage[10.158.0.2:50010,DS-21cbd864-6142-4bfc-9f76-a27af38bf893,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.6:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745713_4889 after checking nodes = [DatanodeInfoWithStorage[10.158.0.5:50010,DS-a1f5fe72-94b9-4bc2-8198-4b57b30de836,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.14:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745608_4784 after checking nodes = [DatanodeInfoWithStorage[10.158.0.24:50010,DS-d40f2af7-438b-440a-a5be-45ada1236747,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.8:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.18:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745351_4527 after checking nodes = [DatanodeInfoWithStorage[10.158.0.8:50010,DS-53687765-660a-42fe-b5ac-38f4c375756e,DISK]], ignoredNodes = null18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745608_4784 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.24:50010,DS-d40f2af7-438b-440a-a5be-45ada1236747,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.24:50010,DS-d40f2af7-438b-440a-a5be-45ada1236747,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745436_4612 after checking nodes = [DatanodeInfoWithStorage[10.158.0.14:50010,DS-ccbb1e07-fc06-47cd-98c0-6ceffd6976a7,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 733.9063858765879 msec.18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745713_4889 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.5:50010,DS-a1f5fe72-94b9-4bc2-8198-4b57b30de836,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.5:50010,DS-a1f5fe72-94b9-4bc2-8198-4b57b30de836,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745704_4880 after checking nodes = [DatanodeInfoWithStorage[10.158.0.6:50010,DS-18c4c728-90f7-43ee-9f31-c72ad9fc366f,DISK]], ignoredNodes = null18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073746134_5310 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.2:50010,DS-21cbd864-6142-4bfc-9f76-a27af38bf893,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.2:50010,DS-21cbd864-6142-4bfc-9f76-a27af38bf893,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745786_4962 after checking nodes = [DatanodeInfoWithStorage[10.158.0.22:50010,DS-4d64a39b-4be1-4fc6-8e97-0a385473f68c,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073745958_5134 after checking nodes = [DatanodeInfoWithStorage[10.158.0.3:50010,DS-0665f31f-46f4-4aed-90ec-8473cc214ff4,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: Failed to connect to /10.158.0.19:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745958_5134 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.3:50010,DS-0665f31f-46f4-4aed-90ec-8473cc214ff4,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.3:50010,DS-0665f31f-46f4-4aed-90ec-8473cc214ff4,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745786_4962 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.22:50010,DS-4d64a39b-4be1-4fc6-8e97-0a385473f68c,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.22:50010,DS-4d64a39b-4be1-4fc6-8e97-0a385473f68c,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1755.705875843023 msec.18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745704_4880 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.6:50010,DS-18c4c728-90f7-43ee-9f31-c72ad9fc366f,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.6:50010,DS-18c4c728-90f7-43ee-9f31-c72ad9fc366f,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2325.8756160482008 msec.18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745436_4612 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.14:50010,DS-ccbb1e07-fc06-47cd-98c0-6ceffd6976a7,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.14:50010,DS-ccbb1e07-fc06-47cd-98c0-6ceffd6976a7,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073745351_4527 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.8:50010,DS-53687765-660a-42fe-b5ac-38f4c375756e,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.8:50010,DS-53687765-660a-42fe-b5ac-38f4c375756e,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073746056_5232 after checking nodes = [DatanodeInfoWithStorage[10.158.0.18:50010,DS-e9fdd138-9612-4dbf-8ef6-f297b34f270a,DISK]], ignoredNodes = null18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1855.151211642441 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 687.747605254808 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2724.195329851299 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1056.6152947137161 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1426.5766865922667 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: No live nodes contain block BP-407389639-10.158.0.1-1544640748903:blk_1073746140_5316 after checking nodes = [DatanodeInfoWithStorage[10.158.0.19:50010,DS-192c637d-95ce-43b6-bd8c-9a7576bf4c6a,DISK]], ignoredNodes = null18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073746056_5232 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.18:50010,DS-e9fdd138-9612-4dbf-8ef6-f297b34f270a,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.18:50010,DS-e9fdd138-9612-4dbf-8ef6-f297b34f270a,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 INFO hdfs.DFSClient: Could not obtain BP-407389639-10.158.0.1-1544640748903:blk_1073746140_5316 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.19:50010,DS-192c637d-95ce-43b6-bd8c-9a7576bf4c6a,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.19:50010,DS-192c637d-95ce-43b6-bd8c-9a7576bf4c6a,DISK]. Will get new block locations from namenode and retry...18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 277.8010434046794 msec.18/12/12 19:47:15 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 628.8695492326697 msec.real\t0m1.919s\nuser\t0m4.560s\nsys\t0m0.399s", :stdout=>"Spent 195ms computing base-splits.Spent 5ms computing TeraScheduler splits.Computing input splits took 200msSampling 10 splits of 874", :status=>255}}
Exp  3, overall time taken is 0 m 1.919 s
Exp  3 termintated at 2018-12-12 20:47:15 +0100

Exp  4 started at  2018-12-12 20:56:05 +0100
