Exp  1 started at  2018-12-13 05:19:58 +0100
Exp 1's result is {"10.158.0.1"=>{:stderr=>"18/12/13 04:20:53 INFO terasort.TeraSort: starting18/12/13 04:20:53 INFO input.FileInputFormat: Total input files to process : 2318/12/13 04:20:55 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/13 04:20:56 INFO mapreduce.JobSubmitter: number of splits:87418/12/13 04:20:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/13 04:20:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544674597629_000118/12/13 04:20:57 INFO impl.YarnClientImpl: Submitted application application_1544674597629_000118/12/13 04:20:57 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544674597629_0001/18/12/13 04:20:57 INFO mapreduce.Job: Running job: job_1544674597629_000118/12/13 04:21:03 INFO mapreduce.Job: Job job_1544674597629_0001 running in uber mode : false18/12/13 04:21:03 INFO mapreduce.Job:  map 0% reduce 0%18/12/13 04:21:15 INFO mapreduce.Job:  map 9% reduce 0%18/12/13 04:21:16 INFO mapreduce.Job:  map 24% reduce 0%18/12/13 04:21:17 INFO mapreduce.Job:  map 31% reduce 0%18/12/13 04:21:24 INFO mapreduce.Job:  map 32% reduce 0%18/12/13 04:21:25 INFO mapreduce.Job:  map 38% reduce 0%18/12/13 04:21:26 INFO mapreduce.Job:  map 53% reduce 0%18/12/13 04:21:27 INFO mapreduce.Job:  map 60% reduce 0%18/12/13 04:21:28 INFO mapreduce.Job:  map 61% reduce 0%18/12/13 04:21:33 INFO mapreduce.Job:  map 62% reduce 4%18/12/13 04:21:34 INFO mapreduce.Job:  map 63% reduce 4%18/12/13 04:21:35 INFO mapreduce.Job:  map 67% reduce 4%18/12/13 04:21:36 INFO mapreduce.Job:  map 81% reduce 4%18/12/13 04:21:37 INFO mapreduce.Job:  map 89% reduce 4%18/12/13 04:21:38 INFO mapreduce.Job:  map 92% reduce 4%18/12/13 04:21:39 INFO mapreduce.Job:  map 92% reduce 6%18/12/13 04:21:40 INFO mapreduce.Job:  map 93% reduce 6%18/12/13 04:21:41 INFO mapreduce.Job:  map 94% reduce 6%18/12/13 04:21:42 INFO mapreduce.Job:  map 95% reduce 6%18/12/13 04:21:43 INFO mapreduce.Job:  map 96% reduce 6%18/12/13 04:21:44 INFO mapreduce.Job:  map 98% reduce 6%18/12/13 04:21:45 INFO mapreduce.Job:  map 98% reduce 8%18/12/13 04:21:46 INFO mapreduce.Job:  map 99% reduce 8%18/12/13 04:21:47 INFO mapreduce.Job:  map 100% reduce 8%18/12/13 04:21:51 INFO mapreduce.Job:  map 100% reduce 10%18/12/13 04:21:57 INFO mapreduce.Job:  map 100% reduce 13%18/12/13 04:22:03 INFO mapreduce.Job:  map 100% reduce 15%18/12/13 04:22:09 INFO mapreduce.Job:  map 100% reduce 18%18/12/13 04:22:15 INFO mapreduce.Job:  map 100% reduce 20%18/12/13 04:22:21 INFO mapreduce.Job:  map 100% reduce 22%18/12/13 04:22:27 INFO mapreduce.Job:  map 100% reduce 25%18/12/13 04:22:33 INFO mapreduce.Job:  map 100% reduce 27%18/12/13 04:22:39 INFO mapreduce.Job:  map 100% reduce 29%18/12/13 04:22:44 INFO mapreduce.Job:  map 100% reduce 30%18/12/13 04:22:45 INFO mapreduce.Job:  map 100% reduce 31%18/12/13 04:22:50 INFO mapreduce.Job:  map 100% reduce 32%18/12/13 04:22:51 INFO mapreduce.Job:  map 100% reduce 33%18/12/13 04:23:14 INFO mapreduce.Job:  map 100% reduce 34%18/12/13 04:23:15 INFO mapreduce.Job:  map 100% reduce 35%18/12/13 04:23:20 INFO mapreduce.Job:  map 100% reduce 36%18/12/13 04:23:21 INFO mapreduce.Job:  map 100% reduce 38%18/12/13 04:23:27 INFO mapreduce.Job:  map 100% reduce 39%18/12/13 04:23:28 INFO mapreduce.Job:  map 100% reduce 44%18/12/13 04:23:33 INFO mapreduce.Job:  map 100% reduce 45%18/12/13 04:23:34 INFO mapreduce.Job:  map 100% reduce 51%18/12/13 04:23:39 INFO mapreduce.Job:  map 100% reduce 52%18/12/13 04:23:40 INFO mapreduce.Job:  map 100% reduce 58%18/12/13 04:23:45 INFO mapreduce.Job:  map 100% reduce 60%18/12/13 04:23:46 INFO mapreduce.Job:  map 100% reduce 64%18/12/13 04:23:51 INFO mapreduce.Job:  map 100% reduce 65%18/12/13 04:23:52 INFO mapreduce.Job:  map 100% reduce 68%18/12/13 04:23:57 INFO mapreduce.Job:  map 100% reduce 69%18/12/13 04:23:58 INFO mapreduce.Job:  map 100% reduce 70%18/12/13 04:24:03 INFO mapreduce.Job:  map 100% reduce 71%18/12/13 04:24:04 INFO mapreduce.Job:  map 100% reduce 72%18/12/13 04:24:09 INFO mapreduce.Job:  map 100% reduce 73%18/12/13 04:24:10 INFO mapreduce.Job:  map 100% reduce 74%18/12/13 04:24:15 INFO mapreduce.Job:  map 100% reduce 75%18/12/13 04:24:16 INFO mapreduce.Job:  map 100% reduce 76%18/12/13 04:24:21 INFO mapreduce.Job:  map 100% reduce 77%18/12/13 04:24:22 INFO mapreduce.Job:  map 100% reduce 78%18/12/13 04:24:27 INFO mapreduce.Job:  map 100% reduce 79%18/12/13 04:24:28 INFO mapreduce.Job:  map 100% reduce 80%18/12/13 04:24:33 INFO mapreduce.Job:  map 100% reduce 81%18/12/13 04:24:34 INFO mapreduce.Job:  map 100% reduce 82%18/12/13 04:24:39 INFO mapreduce.Job:  map 100% reduce 83%18/12/13 04:24:45 INFO mapreduce.Job:  map 100% reduce 85%18/12/13 04:24:51 INFO mapreduce.Job:  map 100% reduce 87%18/12/13 04:24:57 INFO mapreduce.Job:  map 100% reduce 89%18/12/13 04:25:03 INFO mapreduce.Job:  map 100% reduce 91%18/12/13 04:25:09 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 04:25:15 INFO mapreduce.Job:  map 100% reduce 94%18/12/13 04:25:21 INFO mapreduce.Job:  map 100% reduce 96%18/12/13 04:25:27 INFO mapreduce.Job:  map 100% reduce 97%18/12/13 04:25:33 INFO mapreduce.Job:  map 100% reduce 98%18/12/13 04:25:39 INFO mapreduce.Job:  map 100% reduce 99%18/12/13 04:25:45 INFO mapreduce.Job:  map 100% reduce 100%18/12/13 04:25:52 INFO mapreduce.Job: Job job_1544674597629_0001 completed successfully18/12/13 04:25:52 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=381461891572\n\t\tFILE: Number of bytes written=501168086384\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=875\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=865\n\t\tRack-local map tasks=10\n\t\tTotal time spent by all maps in occupied slots (ms)=7800423\n\t\tTotal time spent by all reduces in occupied slots (ms)=2810648\n\t\tTotal time spent by all map tasks (ms)=7800423\n\t\tTotal time spent by all reduce tasks (ms)=2810648\n\t\tTotal vcore-milliseconds taken by all map tasks=7800423\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2810648\n\t\tTotal megabyte-milliseconds taken by all map tasks=7987633152\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=2878103552\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4817568885\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=381877\n\t\tCPU time spent (ms)=13631540\n\t\tPhysical memory (bytes) snapshot=264050196480\n\t\tVirtual memory (bytes) snapshot=1783049306112\n\t\tTotal committed heap usage (bytes)=174726316032\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/13 04:25:52 INFO terasort.TeraSort: donereal\t5m0.449s\nuser\t0m10.244s\nsys\t0m1.126s", :stdout=>"Spent 193ms computing base-splits.Spent 6ms computing TeraScheduler splits.\nComputing input splits took 199msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1157msSpent 1359ms computing partitions.", :status=>0}}
Exp  1, overall time taken is 5 m 0.449 s
Exp  1 termintated at 2018-12-13 05:25:52 +0100

Exp  2 started at  2018-12-13 05:35:49 +0100
Exp 2's result is {"10.158.0.1"=>{:stderr=>"18/12/13 04:36:43 INFO terasort.TeraSort: starting18/12/13 04:36:44 INFO input.FileInputFormat: Total input files to process : 2318/12/13 04:36:45 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/13 04:36:46 INFO mapreduce.JobSubmitter: number of splits:87418/12/13 04:36:46 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/13 04:36:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544675546998_000118/12/13 04:36:47 INFO impl.YarnClientImpl: Submitted application application_1544675546998_000118/12/13 04:36:48 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544675546998_0001/18/12/13 04:36:48 INFO mapreduce.Job: Running job: job_1544675546998_000118/12/13 04:36:54 INFO mapreduce.Job: Job job_1544675546998_0001 running in uber mode : false18/12/13 04:36:54 INFO mapreduce.Job:  map 0% reduce 0%18/12/13 04:37:06 INFO mapreduce.Job:  map 7% reduce 0%18/12/13 04:37:07 INFO mapreduce.Job:  map 20% reduce 0%18/12/13 04:37:08 INFO mapreduce.Job:  map 30% reduce 0%18/12/13 04:37:16 INFO mapreduce.Job:  map 36% reduce 0%18/12/13 04:37:17 INFO mapreduce.Job:  map 49% reduce 0%18/12/13 04:37:18 INFO mapreduce.Job:  map 56% reduce 0%18/12/13 04:37:19 INFO mapreduce.Job:  map 59% reduce 0%18/12/13 04:37:24 INFO mapreduce.Job:  map 59% reduce 4%18/12/13 04:37:25 INFO mapreduce.Job:  map 60% reduce 4%18/12/13 04:37:26 INFO mapreduce.Job:  map 65% reduce 4%18/12/13 04:37:27 INFO mapreduce.Job:  map 76% reduce 4%18/12/13 04:37:28 INFO mapreduce.Job:  map 84% reduce 4%18/12/13 04:37:29 INFO mapreduce.Job:  map 87% reduce 4%18/12/13 04:37:30 INFO mapreduce.Job:  map 88% reduce 6%18/12/13 04:37:31 INFO mapreduce.Job:  map 89% reduce 6%18/12/13 04:37:33 INFO mapreduce.Job:  map 90% reduce 6%18/12/13 04:37:34 INFO mapreduce.Job:  map 92% reduce 6%18/12/13 04:37:35 INFO mapreduce.Job:  map 93% reduce 6%18/12/13 04:37:36 INFO mapreduce.Job:  map 93% reduce 8%18/12/13 04:37:37 INFO mapreduce.Job:  map 94% reduce 8%18/12/13 04:37:38 INFO mapreduce.Job:  map 95% reduce 8%18/12/13 04:37:39 INFO mapreduce.Job:  map 96% reduce 8%18/12/13 04:37:42 INFO mapreduce.Job:  map 96% reduce 10%18/12/13 04:37:48 INFO mapreduce.Job:  map 96% reduce 13%18/12/13 04:37:54 INFO mapreduce.Job:  map 96% reduce 15%18/12/13 04:38:00 INFO mapreduce.Job:  map 96% reduce 18%18/12/13 04:38:06 INFO mapreduce.Job:  map 96% reduce 20%18/12/13 04:38:12 INFO mapreduce.Job:  map 96% reduce 22%18/12/13 04:38:18 INFO mapreduce.Job:  map 96% reduce 25%18/12/13 04:38:24 INFO mapreduce.Job:  map 96% reduce 27%18/12/13 04:38:30 INFO mapreduce.Job:  map 96% reduce 29%18/12/13 04:38:36 INFO mapreduce.Job:  map 96% reduce 31%18/12/13 04:38:42 INFO mapreduce.Job:  map 96% reduce 32%18/12/13 04:41:46 INFO mapreduce.Job:  map 97% reduce 32%18/12/13 04:47:26 INFO mapreduce.Job:  map 98% reduce 32%18/12/13 04:52:38 INFO mapreduce.Job:  map 99% reduce 32%18/12/13 04:53:51 INFO mapreduce.Job:  map 100% reduce 32%18/12/13 04:53:54 INFO mapreduce.Job:  map 100% reduce 33%18/12/13 04:53:55 INFO mapreduce.Job:  map 100% reduce 40%18/12/13 04:54:00 INFO mapreduce.Job:  map 100% reduce 42%18/12/13 04:54:01 INFO mapreduce.Job:  map 100% reduce 63%18/12/13 04:54:06 INFO mapreduce.Job:  map 100% reduce 64%18/12/13 04:54:07 INFO mapreduce.Job:  map 100% reduce 68%18/12/13 04:54:13 INFO mapreduce.Job:  map 100% reduce 70%18/12/13 04:54:19 INFO mapreduce.Job:  map 100% reduce 72%18/12/13 04:54:25 INFO mapreduce.Job:  map 100% reduce 74%18/12/13 04:54:31 INFO mapreduce.Job:  map 100% reduce 76%18/12/13 04:54:37 INFO mapreduce.Job:  map 100% reduce 78%18/12/13 04:54:43 INFO mapreduce.Job:  map 100% reduce 79%18/12/13 04:54:48 INFO mapreduce.Job:  map 100% reduce 80%18/12/13 04:54:49 INFO mapreduce.Job:  map 100% reduce 81%18/12/13 04:54:54 INFO mapreduce.Job:  map 100% reduce 82%18/12/13 04:54:55 INFO mapreduce.Job:  map 100% reduce 83%18/12/13 04:55:00 INFO mapreduce.Job:  map 100% reduce 84%18/12/13 04:55:01 INFO mapreduce.Job:  map 100% reduce 85%18/12/13 04:55:06 INFO mapreduce.Job:  map 100% reduce 86%18/12/13 04:55:07 INFO mapreduce.Job:  map 100% reduce 87%18/12/13 04:55:13 INFO mapreduce.Job:  map 100% reduce 89%18/12/13 04:55:19 INFO mapreduce.Job:  map 100% reduce 91%18/12/13 04:55:25 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 04:55:31 INFO mapreduce.Job:  map 100% reduce 95%18/12/13 04:55:37 INFO mapreduce.Job:  map 100% reduce 96%18/12/13 04:55:42 INFO mapreduce.Job:  map 100% reduce 97%18/12/13 04:55:43 INFO mapreduce.Job:  map 100% reduce 98%18/12/13 04:55:49 INFO mapreduce.Job:  map 100% reduce 99%18/12/13 04:55:53 INFO mapreduce.Job:  map 100% reduce 100%18/12/13 04:56:01 INFO mapreduce.Job: Job job_1544675546998_0001 completed successfully18/12/13 04:56:02 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=386766261396\n\t\tFILE: Number of bytes written=506472456208\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=11\n\t\tLaunched map tasks=885\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=848\n\t\tRack-local map tasks=37\n\t\tTotal time spent by all maps in occupied slots (ms)=52507697\n\t\tTotal time spent by all reduces in occupied slots (ms)=12329482\n\t\tTotal time spent by all map tasks (ms)=52507697\n\t\tTotal time spent by all reduce tasks (ms)=12329482\n\t\tTotal vcore-milliseconds taken by all map tasks=52507697\n\t\tTotal vcore-milliseconds taken by all reduce tasks=12329482\n\t\tTotal megabyte-milliseconds taken by all map tasks=53767881728\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=12625389568\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4868572441\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=391397\n\t\tCPU time spent (ms)=13937010\n\t\tPhysical memory (bytes) snapshot=264299548672\n\t\tVirtual memory (bytes) snapshot=1783180054528\n\t\tTotal committed heap usage (bytes)=174636138496\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/13 04:56:02 INFO terasort.TeraSort: donereal\t19m19.322s\nuser\t0m13.267s\nsys\t0m1.600s", :stdout=>"Spent 180ms computing base-splits.Spent 4ms computing TeraScheduler splits.\nComputing input splits took 185msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1005msSpent 1192ms computing partitions.", :status=>0}}
Exp  2, overall time taken is 19 m 19.322 s
Exp  2 termintated at 2018-12-13 05:56:02 +0100

Exp  3 started at  2018-12-13 06:06:03 +0100
Exp 3's result is {"10.158.0.1"=>{:stderr=>"18/12/13 05:06:58 INFO terasort.TeraSort: starting18/12/13 05:06:59 INFO input.FileInputFormat: Total input files to process : 2318/12/13 05:06:59 ERROR terasort.TeraSort: Cannot create file/output/_partition.lst. Name node is in safe mode.\nThe reported blocks 889 has reached the threshold 0.9990 of total blocks 889. The number of live datanodes 23 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 22 seconds. NamenodeHostName:node1\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1407)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1395)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2278)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2223)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:728)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:413)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.6:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.20:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.5:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745359_4535 after checking nodes = [DatanodeInfoWithStorage[10.158.0.6:50010,DS-96988d15-f5aa-46b0-8bad-1b8ac1a057ce,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.2:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.7:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.10:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.19:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.18:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.15:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745880_5056 after checking nodes = [DatanodeInfoWithStorage[10.158.0.18:50010,DS-c10430b1-ad16-4155-a280-f6bd4a4dfe12,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073746051_5227 after checking nodes = [DatanodeInfoWithStorage[10.158.0.19:50010,DS-c37f1d9d-da66-4bb8-9cea-6523cfd073d7,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745415_4591 after checking nodes = [DatanodeInfoWithStorage[10.158.0.10:50010,DS-502efdbd-9365-4323-a0c1-be7bd3695552,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745712_4888 after checking nodes = [DatanodeInfoWithStorage[10.158.0.7:50010,DS-a1a71ea4-810f-46fb-a89b-43fe1f46e340,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073746133_5309 after checking nodes = [DatanodeInfoWithStorage[10.158.0.2:50010,DS-cd923a20-3e16-457d-bf24-dbab6aa0e928,DISK]], ignoredNodes = null18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745359_4535 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.6:50010,DS-96988d15-f5aa-46b0-8bad-1b8ac1a057ce,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.6:50010,DS-96988d15-f5aa-46b0-8bad-1b8ac1a057ce,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745944_5120 after checking nodes = [DatanodeInfoWithStorage[10.158.0.5:50010,DS-d3d26290-201b-4bb9-85f8-87e71e488331,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745786_4962 after checking nodes = [DatanodeInfoWithStorage[10.158.0.20:50010,DS-3c02d6ad-a37e-4cdf-8e33-8200ad462419,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: Failed to connect to /10.158.0.8:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745786_4962 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.20:50010,DS-3c02d6ad-a37e-4cdf-8e33-8200ad462419,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.20:50010,DS-3c02d6ad-a37e-4cdf-8e33-8200ad462419,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745944_5120 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.5:50010,DS-d3d26290-201b-4bb9-85f8-87e71e488331,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.5:50010,DS-d3d26290-201b-4bb9-85f8-87e71e488331,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 476.05800279148434 msec.18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073746133_5309 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.2:50010,DS-cd923a20-3e16-457d-bf24-dbab6aa0e928,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.2:50010,DS-cd923a20-3e16-457d-bf24-dbab6aa0e928,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745712_4888 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.7:50010,DS-a1a71ea4-810f-46fb-a89b-43fe1f46e340,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.7:50010,DS-a1a71ea4-810f-46fb-a89b-43fe1f46e340,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745415_4591 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.10:50010,DS-502efdbd-9365-4323-a0c1-be7bd3695552,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.10:50010,DS-502efdbd-9365-4323-a0c1-be7bd3695552,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073746051_5227 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.19:50010,DS-c37f1d9d-da66-4bb8-9cea-6523cfd073d7,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.19:50010,DS-c37f1d9d-da66-4bb8-9cea-6523cfd073d7,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745880_5056 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.18:50010,DS-c10430b1-ad16-4155-a280-f6bd4a4dfe12,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.18:50010,DS-c10430b1-ad16-4155-a280-f6bd4a4dfe12,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745605_4781 after checking nodes = [DatanodeInfoWithStorage[10.158.0.15:50010,DS-9f3bc277-8436-4ff9-8f52-27921ffc9ddd,DISK]], ignoredNodes = null18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2699.0462922487677 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 274.95811043378694 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 345.36531709583096 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 655.1824394235998 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 414.82160150428194 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1505.232320764645 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2977.460160719837 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: No live nodes contain block BP-1477486848-10.158.0.1-1544674177832:blk_1073745440_4616 after checking nodes = [DatanodeInfoWithStorage[10.158.0.8:50010,DS-65bf7ba2-bf63-4167-a437-a802e6bf4f23,DISK]], ignoredNodes = null18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745605_4781 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.15:50010,DS-9f3bc277-8436-4ff9-8f52-27921ffc9ddd,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.15:50010,DS-9f3bc277-8436-4ff9-8f52-27921ffc9ddd,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 INFO hdfs.DFSClient: Could not obtain BP-1477486848-10.158.0.1-1544674177832:blk_1073745440_4616 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.8:50010,DS-65bf7ba2-bf63-4167-a437-a802e6bf4f23,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.8:50010,DS-65bf7ba2-bf63-4167-a437-a802e6bf4f23,DISK]. Will get new block locations from namenode and retry...18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1670.3379544721283 msec.18/12/13 05:06:59 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2376.4113862947047 msec.real\t0m1.948s\nuser\t0m4.515s\nsys\t0m0.582s", :stdout=>"Spent 187ms computing base-splits.Spent 6ms computing TeraScheduler splits.\nComputing input splits took 193msSampling 10 splits of 874", :status=>255}}
Exp  3, overall time taken is 0 m 1.948 s
Exp  3 termintated at 2018-12-13 06:06:59 +0100

Exp  4 started at  2018-12-13 06:17:03 +0100
Exp 4's result is {"10.158.0.1"=>{:stderr=>"18/12/13 05:17:57 INFO terasort.TeraSort: starting18/12/13 05:17:58 INFO input.FileInputFormat: Total input files to process : 2318/12/13 05:18:00 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/13 05:18:01 INFO mapreduce.JobSubmitter: number of splits:87418/12/13 05:18:01 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/13 05:18:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544678018136_000118/12/13 05:18:02 INFO impl.YarnClientImpl: Submitted application application_1544678018136_000118/12/13 05:18:02 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544678018136_0001/18/12/13 05:18:02 INFO mapreduce.Job: Running job: job_1544678018136_000118/12/13 05:18:08 INFO mapreduce.Job: Job job_1544678018136_0001 running in uber mode : false18/12/13 05:18:08 INFO mapreduce.Job:  map 0% reduce 0%18/12/13 05:18:26 INFO mapreduce.Job:  map 2% reduce 0%18/12/13 05:18:27 INFO mapreduce.Job:  map 4% reduce 0%18/12/13 05:18:28 INFO mapreduce.Job:  map 5% reduce 0%18/12/13 05:18:32 INFO mapreduce.Job:  map 6% reduce 0%18/12/13 05:18:33 INFO mapreduce.Job:  map 7% reduce 0%18/12/13 05:18:38 INFO mapreduce.Job:  map 8% reduce 0%18/12/13 05:18:39 INFO mapreduce.Job:  map 9% reduce 0%18/12/13 05:18:44 INFO mapreduce.Job:  map 10% reduce 0%18/12/13 05:18:45 INFO mapreduce.Job:  map 12% reduce 0%18/12/13 05:18:50 INFO mapreduce.Job:  map 13% reduce 0%18/12/13 05:18:51 INFO mapreduce.Job:  map 14% reduce 0%18/12/13 05:18:56 INFO mapreduce.Job:  map 15% reduce 0%18/12/13 05:18:57 INFO mapreduce.Job:  map 16% reduce 0%18/12/13 05:19:02 INFO mapreduce.Job:  map 18% reduce 0%18/12/13 05:19:03 INFO mapreduce.Job:  map 19% reduce 0%18/12/13 05:19:08 INFO mapreduce.Job:  map 20% reduce 0%18/12/13 05:19:09 INFO mapreduce.Job:  map 22% reduce 0%18/12/13 05:19:10 INFO mapreduce.Job:  map 23% reduce 0%18/12/13 05:19:11 INFO mapreduce.Job:  map 26% reduce 0%18/12/13 05:19:12 INFO mapreduce.Job:  map 27% reduce 0%18/12/13 05:19:13 INFO mapreduce.Job:  map 28% reduce 0%18/12/13 05:19:15 INFO mapreduce.Job:  map 29% reduce 0%18/12/13 05:19:16 INFO mapreduce.Job:  map 30% reduce 0%18/12/13 05:19:20 INFO mapreduce.Job:  map 31% reduce 0%18/12/13 05:19:26 INFO mapreduce.Job:  map 32% reduce 0%18/12/13 05:19:27 INFO mapreduce.Job:  map 33% reduce 0%18/12/13 05:19:28 INFO mapreduce.Job:  map 35% reduce 3%18/12/13 05:19:33 INFO mapreduce.Job:  map 36% reduce 3%18/12/13 05:19:34 INFO mapreduce.Job:  map 37% reduce 6%18/12/13 05:19:35 INFO mapreduce.Job:  map 38% reduce 6%18/12/13 05:19:39 INFO mapreduce.Job:  map 39% reduce 6%18/12/13 05:19:40 INFO mapreduce.Job:  map 40% reduce 8%18/12/13 05:19:45 INFO mapreduce.Job:  map 41% reduce 8%18/12/13 05:19:46 INFO mapreduce.Job:  map 42% reduce 10%18/12/13 05:19:49 INFO mapreduce.Job:  map 43% reduce 10%18/12/13 05:19:51 INFO mapreduce.Job:  map 44% reduce 10%18/12/13 05:19:52 INFO mapreduce.Job:  map 45% reduce 11%18/12/13 05:19:55 INFO mapreduce.Job:  map 46% reduce 11%18/12/13 05:19:57 INFO mapreduce.Job:  map 47% reduce 11%18/12/13 05:19:58 INFO mapreduce.Job:  map 48% reduce 12%18/12/13 05:20:01 INFO mapreduce.Job:  map 49% reduce 12%18/12/13 05:20:03 INFO mapreduce.Job:  map 50% reduce 12%18/12/13 05:20:04 INFO mapreduce.Job:  map 51% reduce 12%18/12/13 05:20:05 INFO mapreduce.Job:  map 52% reduce 12%18/12/13 05:20:08 INFO mapreduce.Job:  map 53% reduce 12%18/12/13 05:20:09 INFO mapreduce.Job:  map 54% reduce 12%18/12/13 05:20:10 INFO mapreduce.Job:  map 55% reduce 15%18/12/13 05:20:11 INFO mapreduce.Job:  map 56% reduce 15%18/12/13 05:20:16 INFO mapreduce.Job:  map 56% reduce 16%18/12/13 05:20:18 INFO mapreduce.Job:  map 57% reduce 16%18/12/13 05:20:21 INFO mapreduce.Job:  map 58% reduce 16%18/12/13 05:20:22 INFO mapreduce.Job:  map 58% reduce 17%18/12/13 05:20:25 INFO mapreduce.Job:  map 59% reduce 17%18/12/13 05:20:28 INFO mapreduce.Job:  map 60% reduce 18%18/12/13 05:20:32 INFO mapreduce.Job:  map 61% reduce 18%18/12/13 05:20:34 INFO mapreduce.Job:  map 61% reduce 19%18/12/13 05:20:35 INFO mapreduce.Job:  map 62% reduce 19%18/12/13 05:20:37 INFO mapreduce.Job:  map 63% reduce 19%18/12/13 05:20:41 INFO mapreduce.Job:  map 64% reduce 19%18/12/13 05:20:45 INFO mapreduce.Job:  map 65% reduce 19%18/12/13 05:20:48 INFO mapreduce.Job:  map 66% reduce 19%18/12/13 05:20:51 INFO mapreduce.Job:  map 67% reduce 19%18/12/13 05:20:52 INFO mapreduce.Job:  map 67% reduce 20%18/12/13 05:20:56 INFO mapreduce.Job:  map 68% reduce 20%18/12/13 05:20:58 INFO mapreduce.Job:  map 69% reduce 20%18/12/13 05:21:01 INFO mapreduce.Job:  map 70% reduce 20%18/12/13 05:21:04 INFO mapreduce.Job:  map 71% reduce 20%18/12/13 05:21:08 INFO mapreduce.Job:  map 72% reduce 20%18/12/13 05:21:11 INFO mapreduce.Job:  map 73% reduce 20%18/12/13 05:21:14 INFO mapreduce.Job:  map 74% reduce 20%18/12/13 05:21:15 INFO mapreduce.Job:  map 75% reduce 20%18/12/13 05:21:16 INFO mapreduce.Job:  map 75% reduce 21%18/12/13 05:21:17 INFO mapreduce.Job:  map 76% reduce 21%18/12/13 05:21:19 INFO mapreduce.Job:  map 77% reduce 21%18/12/13 05:21:21 INFO mapreduce.Job:  map 78% reduce 21%18/12/13 05:21:22 INFO mapreduce.Job:  map 78% reduce 22%18/12/13 05:21:24 INFO mapreduce.Job:  map 79% reduce 22%18/12/13 05:21:26 INFO mapreduce.Job:  map 80% reduce 22%18/12/13 05:21:29 INFO mapreduce.Job:  map 81% reduce 22%18/12/13 05:21:32 INFO mapreduce.Job:  map 82% reduce 22%18/12/13 05:21:34 INFO mapreduce.Job:  map 82% reduce 23%18/12/13 05:21:37 INFO mapreduce.Job:  map 83% reduce 23%18/12/13 05:21:39 INFO mapreduce.Job:  map 84% reduce 23%18/12/13 05:21:42 INFO mapreduce.Job:  map 85% reduce 23%18/12/13 05:21:45 INFO mapreduce.Job:  map 86% reduce 23%18/12/13 05:21:49 INFO mapreduce.Job:  map 87% reduce 23%18/12/13 05:21:52 INFO mapreduce.Job:  map 87% reduce 24%18/12/13 05:21:54 INFO mapreduce.Job:  map 88% reduce 24%18/12/13 05:21:58 INFO mapreduce.Job:  map 89% reduce 24%18/12/13 05:22:04 INFO mapreduce.Job:  map 90% reduce 24%18/12/13 05:22:08 INFO mapreduce.Job:  map 91% reduce 24%18/12/13 05:22:10 INFO mapreduce.Job:  map 91% reduce 25%18/12/13 05:22:13 INFO mapreduce.Job:  map 92% reduce 25%18/12/13 05:22:16 INFO mapreduce.Job:  map 93% reduce 25%18/12/13 05:22:22 INFO mapreduce.Job:  map 94% reduce 25%18/12/13 05:22:27 INFO mapreduce.Job:  map 95% reduce 25%18/12/13 05:22:31 INFO mapreduce.Job:  map 96% reduce 25%18/12/13 05:22:34 INFO mapreduce.Job:  map 96% reduce 26%18/12/13 05:22:37 INFO mapreduce.Job:  map 97% reduce 26%18/12/13 05:22:41 INFO mapreduce.Job:  map 98% reduce 26%18/12/13 05:22:52 INFO mapreduce.Job:  map 98% reduce 27%18/12/13 05:22:55 INFO mapreduce.Job:  map 99% reduce 27%18/12/13 05:23:21 INFO mapreduce.Job:  map 100% reduce 27%18/12/13 05:23:29 INFO mapreduce.Job:  map 100% reduce 28%18/12/13 05:24:58 INFO mapreduce.Job:  map 100% reduce 29%18/12/13 05:26:52 INFO mapreduce.Job:  map 100% reduce 30%18/12/13 05:28:35 INFO mapreduce.Job:  map 100% reduce 31%18/12/13 05:29:29 INFO mapreduce.Job:  map 100% reduce 32%18/12/13 05:30:35 INFO mapreduce.Job:  map 100% reduce 33%18/12/13 05:32:36 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_m_000208_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_m_000208_0 Timed out after 600 secs18/12/13 05:32:36 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_m_000209_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_m_000209_0 Timed out after 600 secs18/12/13 05:32:47 INFO mapreduce.Job:  map 100% reduce 34%18/12/13 05:32:53 INFO mapreduce.Job:  map 100% reduce 36%18/12/13 05:32:54 INFO mapreduce.Job:  map 100% reduce 38%18/12/13 05:32:59 INFO mapreduce.Job:  map 100% reduce 41%18/12/13 05:33:05 INFO mapreduce.Job:  map 100% reduce 42%18/12/13 05:33:11 INFO mapreduce.Job:  map 100% reduce 43%18/12/13 05:33:29 INFO mapreduce.Job:  map 100% reduce 44%18/12/13 05:33:41 INFO mapreduce.Job:  map 100% reduce 45%18/12/13 05:33:47 INFO mapreduce.Job:  map 100% reduce 46%18/12/13 05:33:59 INFO mapreduce.Job:  map 100% reduce 47%18/12/13 05:34:11 INFO mapreduce.Job:  map 100% reduce 48%18/12/13 05:34:17 INFO mapreduce.Job:  map 100% reduce 49%18/12/13 05:34:29 INFO mapreduce.Job:  map 100% reduce 50%18/12/13 05:34:40 INFO mapreduce.Job:  map 100% reduce 51%18/12/13 05:34:59 INFO mapreduce.Job:  map 100% reduce 52%18/12/13 05:40:13 INFO mapreduce.Job:  map 100% reduce 54%18/12/13 05:40:19 INFO mapreduce.Job:  map 100% reduce 56%18/12/13 05:40:25 INFO mapreduce.Job:  map 100% reduce 57%18/12/13 05:40:31 INFO mapreduce.Job:  map 100% reduce 58%18/12/13 05:43:17 INFO mapreduce.Job:  map 100% reduce 59%18/12/13 05:43:23 INFO mapreduce.Job:  map 100% reduce 60%18/12/13 05:44:24 INFO mapreduce.Job:  map 100% reduce 61%18/12/13 05:44:30 INFO mapreduce.Job:  map 100% reduce 62%18/12/13 05:44:36 INFO mapreduce.Job:  map 100% reduce 64%18/12/13 05:45:06 INFO mapreduce.Job:  map 100% reduce 66%18/12/13 05:45:12 INFO mapreduce.Job:  map 100% reduce 69%18/12/13 05:45:18 INFO mapreduce.Job:  map 100% reduce 70%18/12/13 05:45:30 INFO mapreduce.Job:  map 100% reduce 71%18/12/13 05:45:42 INFO mapreduce.Job:  map 100% reduce 72%18/12/13 05:45:49 INFO mapreduce.Job:  map 100% reduce 73%18/12/13 05:46:00 INFO mapreduce.Job:  map 100% reduce 74%18/12/13 05:46:07 INFO mapreduce.Job:  map 100% reduce 75%18/12/13 05:46:18 INFO mapreduce.Job:  map 100% reduce 76%18/12/13 05:46:25 INFO mapreduce.Job:  map 100% reduce 77%18/12/13 05:46:36 INFO mapreduce.Job:  map 100% reduce 78%18/12/13 05:46:43 INFO mapreduce.Job:  map 100% reduce 79%18/12/13 05:46:54 INFO mapreduce.Job:  map 100% reduce 80%18/12/13 05:47:01 INFO mapreduce.Job:  map 100% reduce 81%18/12/13 05:47:13 INFO mapreduce.Job:  map 100% reduce 82%18/12/13 05:51:29 INFO mapreduce.Job:  map 100% reduce 83%18/12/13 05:52:17 INFO mapreduce.Job:  map 100% reduce 84%18/12/13 05:54:10 INFO mapreduce.Job:  map 100% reduce 85%18/12/13 05:55:05 INFO mapreduce.Job:  map 100% reduce 86%18/12/13 05:55:12 INFO mapreduce.Job:  map 100% reduce 87%18/12/13 05:55:24 INFO mapreduce.Job:  map 100% reduce 88%18/12/13 05:55:35 INFO mapreduce.Job:  map 100% reduce 89%18/12/13 05:55:46 INFO mapreduce.Job:  map 100% reduce 90%18/12/13 05:55:54 INFO mapreduce.Job:  map 100% reduce 91%18/12/13 05:56:06 INFO mapreduce.Job:  map 100% reduce 92%18/12/13 05:56:18 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 05:57:01 INFO mapreduce.Job:  map 100% reduce 94%18/12/13 05:57:07 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_r_000009_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_r_000009_0 Timed out after 600 secsContainer killed by the ApplicationMaster.\nSent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 12703 as user hadoop for container container_1544678018136_0001_01_000383, result=success\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 14318/12/13 05:57:08 INFO mapreduce.Job:  map 100% reduce 91%18/12/13 05:57:55 INFO mapreduce.Job:  map 100% reduce 92%18/12/13 05:59:06 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 06:00:13 INFO mapreduce.Job:  map 100% reduce 94%18/12/13 06:04:35 INFO mapreduce.Job:  map 100% reduce 95%18/12/13 06:04:47 INFO mapreduce.Job:  map 100% reduce 96%18/12/13 06:04:53 INFO mapreduce.Job:  map 100% reduce 97%18/12/13 06:09:23 INFO mapreduce.Job:  map 100% reduce 98%18/12/13 06:09:53 INFO mapreduce.Job:  map 100% reduce 99%18/12/13 06:10:06 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_r_000003_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_r_000003_0 Timed out after 600 secsContainer killed by the ApplicationMaster.\nSent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 12686 as user hadoop for container container_1544678018136_0001_01_000377, result=success\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 14318/12/13 06:10:06 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_r_000002_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_r_000002_0 Timed out after 600 secs\nContainer killed by the ApplicationMaster.\nSent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 12689 as user hadoop for container container_1544678018136_0001_01_000376, result=success\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 14318/12/13 06:10:07 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 06:10:29 INFO mapreduce.Job:  map 100% reduce 94%18/12/13 06:11:07 INFO mapreduce.Job: Task Id : attempt_1544678018136_0001_r_000000_0, Status : FAILEDAttemptID:attempt_1544678018136_0001_r_000000_0 Timed out after 600 secs\nContainer killed by the ApplicationMaster.\nSent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 12308 as user hadoop for container container_1544678018136_0001_01_000374, result=success\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 14318/12/13 06:11:08 INFO mapreduce.Job:  map 100% reduce 88%18/12/13 06:17:45 INFO mapreduce.Job:  map 100% reduce 89%18/12/13 06:18:00 INFO mapreduce.Job:  map 100% reduce 90%18/12/13 06:18:03 INFO mapreduce.Job:  map 100% reduce 91%18/12/13 06:18:06 INFO mapreduce.Job:  map 100% reduce 92%18/12/13 06:18:33 INFO mapreduce.Job:  map 100% reduce 93%18/12/13 06:19:09 INFO mapreduce.Job:  map 100% reduce 94%18/12/13 06:20:13 INFO mapreduce.Job:  map 100% reduce 95%18/12/13 06:20:43 INFO mapreduce.Job:  map 100% reduce 96%18/12/13 06:21:13 INFO mapreduce.Job:  map 100% reduce 97%18/12/13 06:21:29 INFO mapreduce.Job:  map 100% reduce 98%18/12/13 06:21:59 INFO mapreduce.Job:  map 100% reduce 99%18/12/13 06:22:29 INFO mapreduce.Job:  map 100% reduce 100%18/12/13 06:24:45 INFO mapreduce.Job: Job job_1544678018136_0001 completed successfully18/12/13 06:24:45 INFO mapreduce.Job: Counters: 55\n\tFile System Counters\n\t\tFILE: Number of bytes read=382929556316\n\t\tFILE: Number of bytes written=502635751128\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tFailed map tasks=2\n\t\tFailed reduce tasks=4\n\t\tKilled map tasks=5\n\t\tKilled reduce tasks=7\n\t\tLaunched map tasks=881\n\t\tLaunched reduce tasks=22\n\t\tOther local map tasks=3\n\t\tData-local map tasks=829\n\t\tRack-local map tasks=49\n\t\tTotal time spent by all maps in occupied slots (ms)=56423774\n\t\tTotal time spent by all reduces in occupied slots (ms)=43183945\n\t\tTotal time spent by all map tasks (ms)=56423774\n\t\tTotal time spent by all reduce tasks (ms)=43183945\n\t\tTotal vcore-milliseconds taken by all map tasks=56423774\n\t\tTotal vcore-milliseconds taken by all reduce tasks=43183945\n\t\tTotal megabyte-milliseconds taken by all map tasks=57777944576\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=44220359680\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4831681046\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=9\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=295062\n\t\tCPU time spent (ms)=13399420\n\t\tPhysical memory (bytes) snapshot=263948926976\n\t\tVirtual memory (bytes) snapshot=1782813802496\n\t\tTotal committed heap usage (bytes)=174289059840\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=1\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/13 06:24:45 INFO terasort.TeraSort: donereal\t66m48.141s\nuser\t0m19.580s\nsys\t0m3.656s", :stdout=>"Spent 200ms computing base-splits.Spent 6ms computing TeraScheduler splits.\nComputing input splits took 206msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1237msSpent 1447ms computing partitions.", :status=>0}}
Exp  4, overall time taken is 66 m 48.141 s
Exp  4 termintated at 2018-12-13 07:24:45 +0100

Exp  5 started at  2018-12-13 07:37:39 +0100
