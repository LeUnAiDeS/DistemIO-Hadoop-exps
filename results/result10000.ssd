Exp  1 started at  2018-12-12 22:16:17 +0100
Exp 1's result is {"10.158.0.1"=>{:stderr=>"18/12/12 21:17:11 INFO terasort.TeraSort: starting18/12/12 21:17:12 INFO input.FileInputFormat: Total input files to process : 2318/12/12 21:17:14 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/12 21:17:14 INFO mapreduce.JobSubmitter: number of splits:87418/12/12 21:17:14 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/12 21:17:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544649176489_000118/12/12 21:17:15 INFO impl.YarnClientImpl: Submitted application application_1544649176489_000118/12/12 21:17:15 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544649176489_0001/18/12/12 21:17:15 INFO mapreduce.Job: Running job: job_1544649176489_000118/12/12 21:17:21 INFO mapreduce.Job: Job job_1544649176489_0001 running in uber mode : false18/12/12 21:17:21 INFO mapreduce.Job:  map 0% reduce 0%18/12/12 21:17:33 INFO mapreduce.Job:  map 9% reduce 0%18/12/12 21:17:34 INFO mapreduce.Job:  map 26% reduce 0%18/12/12 21:17:36 INFO mapreduce.Job:  map 31% reduce 0%18/12/12 21:17:43 INFO mapreduce.Job:  map 32% reduce 0%18/12/12 21:17:44 INFO mapreduce.Job:  map 37% reduce 0%18/12/12 21:17:45 INFO mapreduce.Job:  map 54% reduce 0%18/12/12 21:17:46 INFO mapreduce.Job:  map 61% reduce 0%18/12/12 21:17:52 INFO mapreduce.Job:  map 62% reduce 4%18/12/12 21:17:53 INFO mapreduce.Job:  map 63% reduce 4%18/12/12 21:17:54 INFO mapreduce.Job:  map 68% reduce 4%18/12/12 21:17:55 INFO mapreduce.Job:  map 79% reduce 4%18/12/12 21:17:56 INFO mapreduce.Job:  map 89% reduce 4%18/12/12 21:17:57 INFO mapreduce.Job:  map 92% reduce 4%18/12/12 21:17:58 INFO mapreduce.Job:  map 92% reduce 6%18/12/12 21:17:59 INFO mapreduce.Job:  map 93% reduce 6%18/12/12 21:18:00 INFO mapreduce.Job:  map 94% reduce 6%18/12/12 21:18:01 INFO mapreduce.Job:  map 95% reduce 6%18/12/12 21:18:02 INFO mapreduce.Job:  map 97% reduce 6%18/12/12 21:18:03 INFO mapreduce.Job:  map 98% reduce 6%18/12/12 21:18:04 INFO mapreduce.Job:  map 99% reduce 8%18/12/12 21:18:05 INFO mapreduce.Job:  map 100% reduce 8%18/12/12 21:18:10 INFO mapreduce.Job:  map 100% reduce 11%18/12/12 21:18:16 INFO mapreduce.Job:  map 100% reduce 13%18/12/12 21:18:22 INFO mapreduce.Job:  map 100% reduce 15%18/12/12 21:18:28 INFO mapreduce.Job:  map 100% reduce 18%18/12/12 21:18:34 INFO mapreduce.Job:  map 100% reduce 20%18/12/12 21:18:40 INFO mapreduce.Job:  map 100% reduce 23%18/12/12 21:18:46 INFO mapreduce.Job:  map 100% reduce 25%18/12/12 21:18:52 INFO mapreduce.Job:  map 100% reduce 27%18/12/12 21:18:58 INFO mapreduce.Job:  map 100% reduce 29%18/12/12 21:19:04 INFO mapreduce.Job:  map 100% reduce 31%18/12/12 21:19:10 INFO mapreduce.Job:  map 100% reduce 33%18/12/12 21:19:34 INFO mapreduce.Job:  map 100% reduce 35%18/12/12 21:19:40 INFO mapreduce.Job:  map 100% reduce 39%18/12/12 21:19:46 INFO mapreduce.Job:  map 100% reduce 45%18/12/12 21:19:52 INFO mapreduce.Job:  map 100% reduce 52%18/12/12 21:19:57 INFO mapreduce.Job:  map 100% reduce 54%18/12/12 21:19:58 INFO mapreduce.Job:  map 100% reduce 59%18/12/12 21:20:03 INFO mapreduce.Job:  map 100% reduce 60%18/12/12 21:20:04 INFO mapreduce.Job:  map 100% reduce 64%18/12/12 21:20:09 INFO mapreduce.Job:  map 100% reduce 65%18/12/12 21:20:10 INFO mapreduce.Job:  map 100% reduce 68%18/12/12 21:20:15 INFO mapreduce.Job:  map 100% reduce 69%18/12/12 21:20:16 INFO mapreduce.Job:  map 100% reduce 71%18/12/12 21:20:22 INFO mapreduce.Job:  map 100% reduce 73%18/12/12 21:20:28 INFO mapreduce.Job:  map 100% reduce 75%18/12/12 21:20:34 INFO mapreduce.Job:  map 100% reduce 77%18/12/12 21:20:40 INFO mapreduce.Job:  map 100% reduce 79%18/12/12 21:20:46 INFO mapreduce.Job:  map 100% reduce 80%18/12/12 21:20:51 INFO mapreduce.Job:  map 100% reduce 81%18/12/12 21:20:52 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 21:20:57 INFO mapreduce.Job:  map 100% reduce 83%18/12/12 21:20:58 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 21:21:03 INFO mapreduce.Job:  map 100% reduce 85%18/12/12 21:21:04 INFO mapreduce.Job:  map 100% reduce 86%18/12/12 21:21:09 INFO mapreduce.Job:  map 100% reduce 87%18/12/12 21:21:10 INFO mapreduce.Job:  map 100% reduce 88%18/12/12 21:21:15 INFO mapreduce.Job:  map 100% reduce 89%18/12/12 21:21:16 INFO mapreduce.Job:  map 100% reduce 90%18/12/12 21:21:21 INFO mapreduce.Job:  map 100% reduce 91%18/12/12 21:21:22 INFO mapreduce.Job:  map 100% reduce 92%18/12/12 21:21:27 INFO mapreduce.Job:  map 100% reduce 94%18/12/12 21:21:34 INFO mapreduce.Job:  map 100% reduce 95%18/12/12 21:21:40 INFO mapreduce.Job:  map 100% reduce 96%18/12/12 21:21:45 INFO mapreduce.Job:  map 100% reduce 97%18/12/12 21:21:46 INFO mapreduce.Job:  map 100% reduce 98%18/12/12 21:21:52 INFO mapreduce.Job:  map 100% reduce 99%18/12/12 21:22:04 INFO mapreduce.Job:  map 100% reduce 100%18/12/12 21:22:15 INFO mapreduce.Job: Job job_1544649176489_0001 completed successfully18/12/12 21:22:15 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=381457620292\n\t\tFILE: Number of bytes written=501163815104\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=1\n\t\tLaunched map tasks=874\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=862\n\t\tRack-local map tasks=12\n\t\tTotal time spent by all maps in occupied slots (ms)=7787204\n\t\tTotal time spent by all reduces in occupied slots (ms)=2771417\n\t\tTotal time spent by all map tasks (ms)=7787204\n\t\tTotal time spent by all reduce tasks (ms)=2771417\n\t\tTotal vcore-milliseconds taken by all map tasks=7787204\n\t\tTotal vcore-milliseconds taken by all reduce tasks=2771417\n\t\tTotal megabyte-milliseconds taken by all map tasks=7974096896\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=2837931008\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4817527815\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=398209\n\t\tCPU time spent (ms)=13888550\n\t\tPhysical memory (bytes) snapshot=264347578368\n\t\tVirtual memory (bytes) snapshot=1782990942208\n\t\tTotal committed heap usage (bytes)=174478327808\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/12 21:22:15 INFO terasort.TeraSort: donereal\t5m4.298s\nuser\t0m10.861s\nsys\t0m1.089s", :stdout=>"Spent 192ms computing base-splits.Spent 5ms computing TeraScheduler splits.Computing input splits took 198msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1199msSpent 1399ms computing partitions.", :status=>0}}
Exp  1, overall time taken is 5 m 4.298 s
Exp  1 termintated at 2018-12-12 22:22:15 +0100

Exp  2 started at  2018-12-12 22:32:10 +0100
Exp 2's result is {"10.158.0.1"=>{:stderr=>"18/12/12 21:33:05 INFO terasort.TeraSort: starting18/12/12 21:33:05 INFO input.FileInputFormat: Total input files to process : 2318/12/12 21:33:07 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/12 21:33:08 INFO mapreduce.JobSubmitter: number of splits:87418/12/12 21:33:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/12 21:33:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544650128589_000118/12/12 21:33:09 INFO impl.YarnClientImpl: Submitted application application_1544650128589_000118/12/12 21:33:09 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544650128589_0001/18/12/12 21:33:09 INFO mapreduce.Job: Running job: job_1544650128589_000118/12/12 21:33:15 INFO mapreduce.Job: Job job_1544650128589_0001 running in uber mode : false18/12/12 21:33:15 INFO mapreduce.Job:  map 0% reduce 0%18/12/12 21:33:27 INFO mapreduce.Job:  map 1% reduce 0%18/12/12 21:33:28 INFO mapreduce.Job:  map 15% reduce 0%18/12/12 21:33:29 INFO mapreduce.Job:  map 28% reduce 0%18/12/12 21:33:30 INFO mapreduce.Job:  map 30% reduce 0%18/12/12 21:33:37 INFO mapreduce.Job:  map 32% reduce 0%18/12/12 21:33:38 INFO mapreduce.Job:  map 39% reduce 0%18/12/12 21:33:39 INFO mapreduce.Job:  map 52% reduce 0%18/12/12 21:33:40 INFO mapreduce.Job:  map 59% reduce 0%18/12/12 21:33:45 INFO mapreduce.Job:  map 59% reduce 1%18/12/12 21:33:46 INFO mapreduce.Job:  map 59% reduce 4%18/12/12 21:33:47 INFO mapreduce.Job:  map 62% reduce 4%18/12/12 21:33:48 INFO mapreduce.Job:  map 68% reduce 4%18/12/12 21:33:49 INFO mapreduce.Job:  map 78% reduce 4%18/12/12 21:33:50 INFO mapreduce.Job:  map 86% reduce 4%18/12/12 21:33:51 INFO mapreduce.Job:  map 87% reduce 4%18/12/12 21:33:52 INFO mapreduce.Job:  map 88% reduce 6%18/12/12 21:33:53 INFO mapreduce.Job:  map 89% reduce 6%18/12/12 21:33:54 INFO mapreduce.Job:  map 90% reduce 6%18/12/12 21:33:55 INFO mapreduce.Job:  map 91% reduce 6%18/12/12 21:33:56 INFO mapreduce.Job:  map 93% reduce 6%18/12/12 21:33:57 INFO mapreduce.Job:  map 93% reduce 7%18/12/12 21:33:58 INFO mapreduce.Job:  map 94% reduce 8%18/12/12 21:33:59 INFO mapreduce.Job:  map 95% reduce 8%18/12/12 21:34:00 INFO mapreduce.Job:  map 96% reduce 8%18/12/12 21:34:03 INFO mapreduce.Job:  map 96% reduce 9%18/12/12 21:34:04 INFO mapreduce.Job:  map 96% reduce 10%18/12/12 21:34:09 INFO mapreduce.Job:  map 96% reduce 11%18/12/12 21:34:10 INFO mapreduce.Job:  map 96% reduce 13%18/12/12 21:34:15 INFO mapreduce.Job:  map 96% reduce 14%18/12/12 21:34:16 INFO mapreduce.Job:  map 96% reduce 15%18/12/12 21:34:21 INFO mapreduce.Job:  map 96% reduce 16%18/12/12 21:34:22 INFO mapreduce.Job:  map 96% reduce 18%18/12/12 21:34:27 INFO mapreduce.Job:  map 96% reduce 19%18/12/12 21:34:28 INFO mapreduce.Job:  map 96% reduce 20%18/12/12 21:34:33 INFO mapreduce.Job:  map 96% reduce 21%18/12/12 21:34:34 INFO mapreduce.Job:  map 96% reduce 23%18/12/12 21:34:40 INFO mapreduce.Job:  map 96% reduce 25%18/12/12 21:34:45 INFO mapreduce.Job:  map 96% reduce 26%18/12/12 21:34:46 INFO mapreduce.Job:  map 96% reduce 28%18/12/12 21:34:52 INFO mapreduce.Job:  map 96% reduce 30%18/12/12 21:34:58 INFO mapreduce.Job:  map 96% reduce 31%18/12/12 21:35:04 INFO mapreduce.Job:  map 96% reduce 32%18/12/12 21:38:08 INFO mapreduce.Job:  map 97% reduce 32%18/12/12 21:43:48 INFO mapreduce.Job:  map 98% reduce 32%18/12/12 21:49:02 INFO mapreduce.Job:  map 99% reduce 32%18/12/12 21:50:12 INFO mapreduce.Job:  map 100% reduce 32%18/12/12 21:50:16 INFO mapreduce.Job:  map 100% reduce 35%18/12/12 21:50:17 INFO mapreduce.Job:  map 100% reduce 42%18/12/12 21:50:22 INFO mapreduce.Job:  map 100% reduce 51%18/12/12 21:50:23 INFO mapreduce.Job:  map 100% reduce 66%18/12/12 21:50:28 INFO mapreduce.Job:  map 100% reduce 67%18/12/12 21:50:29 INFO mapreduce.Job:  map 100% reduce 68%18/12/12 21:50:34 INFO mapreduce.Job:  map 100% reduce 69%18/12/12 21:50:35 INFO mapreduce.Job:  map 100% reduce 70%18/12/12 21:50:40 INFO mapreduce.Job:  map 100% reduce 71%18/12/12 21:50:41 INFO mapreduce.Job:  map 100% reduce 72%18/12/12 21:50:46 INFO mapreduce.Job:  map 100% reduce 73%18/12/12 21:50:47 INFO mapreduce.Job:  map 100% reduce 74%18/12/12 21:50:52 INFO mapreduce.Job:  map 100% reduce 75%18/12/12 21:50:53 INFO mapreduce.Job:  map 100% reduce 76%18/12/12 21:50:58 INFO mapreduce.Job:  map 100% reduce 77%18/12/12 21:50:59 INFO mapreduce.Job:  map 100% reduce 78%18/12/12 21:51:04 INFO mapreduce.Job:  map 100% reduce 79%18/12/12 21:51:05 INFO mapreduce.Job:  map 100% reduce 80%18/12/12 21:51:10 INFO mapreduce.Job:  map 100% reduce 81%18/12/12 21:51:11 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 21:51:16 INFO mapreduce.Job:  map 100% reduce 83%18/12/12 21:51:17 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 21:51:22 INFO mapreduce.Job:  map 100% reduce 85%18/12/12 21:51:23 INFO mapreduce.Job:  map 100% reduce 86%18/12/12 21:51:28 INFO mapreduce.Job:  map 100% reduce 87%18/12/12 21:51:29 INFO mapreduce.Job:  map 100% reduce 88%18/12/12 21:51:34 INFO mapreduce.Job:  map 100% reduce 89%18/12/12 21:51:35 INFO mapreduce.Job:  map 100% reduce 90%18/12/12 21:51:40 INFO mapreduce.Job:  map 100% reduce 91%18/12/12 21:51:41 INFO mapreduce.Job:  map 100% reduce 92%18/12/12 21:51:46 INFO mapreduce.Job:  map 100% reduce 93%18/12/12 21:51:47 INFO mapreduce.Job:  map 100% reduce 94%18/12/12 21:51:52 INFO mapreduce.Job:  map 100% reduce 95%18/12/12 21:51:53 INFO mapreduce.Job:  map 100% reduce 96%18/12/12 21:51:59 INFO mapreduce.Job:  map 100% reduce 97%18/12/12 21:52:05 INFO mapreduce.Job:  map 100% reduce 98%18/12/12 21:52:06 INFO mapreduce.Job:  map 100% reduce 99%18/12/12 21:52:12 INFO mapreduce.Job:  map 100% reduce 100%18/12/12 21:52:17 INFO mapreduce.Job: Job job_1544650128589_0001 completed successfully18/12/12 21:52:17 INFO mapreduce.Job: Counters: 51\n\tFile System Counters\n\t\tFILE: Number of bytes read=386753783684\n\t\tFILE: Number of bytes written=506459978496\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tKilled map tasks=11\n\t\tLaunched map tasks=885\n\t\tLaunched reduce tasks=11\n\t\tData-local map tasks=844\n\t\tRack-local map tasks=41\n\t\tTotal time spent by all maps in occupied slots (ms)=52477401\n\t\tTotal time spent by all reduces in occupied slots (ms)=12291883\n\t\tTotal time spent by all map tasks (ms)=52477401\n\t\tTotal time spent by all reduce tasks (ms)=12291883\n\t\tTotal vcore-milliseconds taken by all map tasks=52477401\n\t\tTotal vcore-milliseconds taken by all reduce tasks=12291883\n\t\tTotal megabyte-milliseconds taken by all map tasks=53736858624\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=12586888192\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4868452463\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=388756\n\t\tCPU time spent (ms)=13901600\n\t\tPhysical memory (bytes) snapshot=264538947584\n\t\tVirtual memory (bytes) snapshot=1783150780416\n\t\tTotal committed heap usage (bytes)=174382383104\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/12 21:52:17 INFO terasort.TeraSort: donereal\t19m12.950s\nuser\t0m13.334s\nsys\t0m1.617s", :stdout=>"Spent 190ms computing base-splits.Spent 5ms computing TeraScheduler splits.\nComputing input splits took 195msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1507msSpent 1706ms computing partitions.", :status=>0}}
Exp  2, overall time taken is 19 m 12.950 s
Exp  2 termintated at 2018-12-12 22:52:17 +0100

Exp  3 started at  2018-12-12 23:02:18 +0100
Exp 3's result is {"10.158.0.1"=>{:stderr=>"18/12/12 22:03:12 INFO terasort.TeraSort: starting18/12/12 22:03:13 INFO input.FileInputFormat: Total input files to process : 2318/12/12 22:03:13 ERROR terasort.TeraSort: Cannot create file/output/_partition.lst. Name node is in safe mode.\nThe reported blocks 889 has reached the threshold 0.9990 of total blocks 889. The number of live datanodes 23 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 21 seconds. NamenodeHostName:node1\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1407)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1395)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2278)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2223)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:728)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:413)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.10:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.11:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.14:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.16:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.15:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073746051_5227 after checking nodes = [DatanodeInfoWithStorage[10.158.0.11:50010,DS-827b76b8-4630-431b-bb25-14dbb5d0e821,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.6:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.20:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745787_4963 after checking nodes = [DatanodeInfoWithStorage[10.158.0.10:50010,DS-b5a71841-b873-4b48-99df-046bb79f72e8,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.12:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.22:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: Failed to connect to /10.158.0.23:50010 for block, add to deadNodes and continue. java.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \njava.io.IOException: Failed on local exception: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.; Host Details : local host is: \"node1/10.158.0.1\"; destination host is: \"node1\":9000; \n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:782)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1435)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)\nCaused by: java.io.InterruptedIOException: Interrupted while waiting for IO on channel java.nio.channels.SocketChannel[connected local=/10.158.0.1:44062 remote=node1/10.158.0.1:9000]. 60000 millis timeout left.\n\tat org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:342)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)\n\tat org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:133)\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat java.io.FilterInputStream.read(FilterInputStream.java:83)\n\tat org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:554)\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745448_4624 after checking nodes = [DatanodeInfoWithStorage[10.158.0.22:50010,DS-e05e5f90-91fb-4200-ab51-429c4cbef414,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745368_4544 after checking nodes = [DatanodeInfoWithStorage[10.158.0.12:50010,DS-72abf4d3-7c29-4cb4-8bec-06b869a44eb2,DISK]], ignoredNodes = null18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745787_4963 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.10:50010,DS-b5a71841-b873-4b48-99df-046bb79f72e8,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.10:50010,DS-b5a71841-b873-4b48-99df-046bb79f72e8,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745433_4609 after checking nodes = [DatanodeInfoWithStorage[10.158.0.20:50010,DS-e64835b7-4bda-4195-b4a5-65ae1ab3a788,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745701_4877 after checking nodes = [DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK]], ignoredNodes = null18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073746051_5227 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.11:50010,DS-827b76b8-4630-431b-bb25-14dbb5d0e821,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.11:50010,DS-827b76b8-4630-431b-bb25-14dbb5d0e821,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745968_5144 after checking nodes = [DatanodeInfoWithStorage[10.158.0.15:50010,DS-c12cd08c-0f8e-406a-8729-a86148642889,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745906_5082 after checking nodes = [DatanodeInfoWithStorage[10.158.0.16:50010,DS-9e83eca9-5264-48b3-bb8d-06996792566f,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073745599_4775 after checking nodes = [DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK]], ignoredNodes = null18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745906_5082 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.16:50010,DS-9e83eca9-5264-48b3-bb8d-06996792566f,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.16:50010,DS-9e83eca9-5264-48b3-bb8d-06996792566f,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745968_5144 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.15:50010,DS-c12cd08c-0f8e-406a-8729-a86148642889,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.15:50010,DS-c12cd08c-0f8e-406a-8729-a86148642889,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 710.84944870543 msec.18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745701_4877 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745433_4609 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.20:50010,DS-e64835b7-4bda-4195-b4a5-65ae1ab3a788,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.20:50010,DS-e64835b7-4bda-4195-b4a5-65ae1ab3a788,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1768.4356907308288 msec.18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745368_4544 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.12:50010,DS-72abf4d3-7c29-4cb4-8bec-06b869a44eb2,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.12:50010,DS-72abf4d3-7c29-4cb4-8bec-06b869a44eb2,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745448_4624 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.22:50010,DS-e05e5f90-91fb-4200-ab51-429c4cbef414,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.22:50010,DS-e05e5f90-91fb-4200-ab51-429c4cbef414,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073746143_5319 after checking nodes = [DatanodeInfoWithStorage[10.158.0.23:50010,DS-3079e3e3-16d5-4de0-947d-05d1404bfa7b,DISK]], ignoredNodes = null18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 229.81455036477038 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1346.8890511647444 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 256.0659277468692 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1702.8062097955292 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 558.4393137142578 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1189.3624781704316 msec.18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073745599_4775 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073746143_5319 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.23:50010,DS-3079e3e3-16d5-4de0-947d-05d1404bfa7b,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.23:50010,DS-3079e3e3-16d5-4de0-947d-05d1404bfa7b,DISK]. Will get new block locations from namenode and retry...18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2163.0592766411 msec.18/12/12 22:03:13 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 168.49913796921135 msec.real\t0m1.947s\nuser\t0m4.418s\nsys\t0m0.699s", :stdout=>"Spent 192ms computing base-splits.Spent 5ms computing TeraScheduler splits.\nComputing input splits took 198msSampling 10 splits of 874", :status=>255}}
Exp  3, overall time taken is 0 m 1.947 s
Exp  3 termintated at 2018-12-12 23:03:13 +0100

Exp  4 started at  2018-12-12 23:13:23 +0100
Exp 4's result is {"10.158.0.1"=>{:stderr=>"18/12/12 22:14:17 INFO terasort.TeraSort: starting18/12/12 22:14:18 INFO input.FileInputFormat: Total input files to process : 2318/12/12 22:14:19 INFO client.RMProxy: Connecting to ResourceManager at node1/10.158.0.1:803218/12/12 22:14:20 INFO mapreduce.JobSubmitter: number of splits:87418/12/12 22:14:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces18/12/12 22:14:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1544652597546_000118/12/12 22:14:21 INFO impl.YarnClientImpl: Submitted application application_1544652597546_000118/12/12 22:14:22 INFO mapreduce.Job: The url to track the job: http://node1:8088/proxy/application_1544652597546_0001/18/12/12 22:14:22 INFO mapreduce.Job: Running job: job_1544652597546_000118/12/12 22:14:28 INFO mapreduce.Job: Job job_1544652597546_0001 running in uber mode : false18/12/12 22:14:28 INFO mapreduce.Job:  map 0% reduce 0%18/12/12 22:14:47 INFO mapreduce.Job:  map 2% reduce 0%18/12/12 22:14:48 INFO mapreduce.Job:  map 4% reduce 0%18/12/12 22:14:49 INFO mapreduce.Job:  map 5% reduce 0%18/12/12 22:14:53 INFO mapreduce.Job:  map 6% reduce 0%18/12/12 22:14:54 INFO mapreduce.Job:  map 7% reduce 0%18/12/12 22:14:59 INFO mapreduce.Job:  map 8% reduce 0%18/12/12 22:15:00 INFO mapreduce.Job:  map 9% reduce 0%18/12/12 22:15:05 INFO mapreduce.Job:  map 10% reduce 0%18/12/12 22:15:06 INFO mapreduce.Job:  map 12% reduce 0%18/12/12 22:15:11 INFO mapreduce.Job:  map 13% reduce 0%18/12/12 22:15:12 INFO mapreduce.Job:  map 14% reduce 0%18/12/12 22:15:17 INFO mapreduce.Job:  map 15% reduce 0%18/12/12 22:15:18 INFO mapreduce.Job:  map 16% reduce 0%18/12/12 22:15:22 INFO mapreduce.Job:  map 17% reduce 0%18/12/12 22:15:23 INFO mapreduce.Job:  map 18% reduce 0%18/12/12 22:15:24 INFO mapreduce.Job:  map 19% reduce 0%18/12/12 22:15:29 INFO mapreduce.Job:  map 20% reduce 0%18/12/12 22:15:30 INFO mapreduce.Job:  map 22% reduce 0%18/12/12 22:15:31 INFO mapreduce.Job:  map 23% reduce 0%18/12/12 22:15:32 INFO mapreduce.Job:  map 25% reduce 0%18/12/12 22:15:33 INFO mapreduce.Job:  map 26% reduce 0%18/12/12 22:15:35 INFO mapreduce.Job:  map 28% reduce 0%18/12/12 22:15:36 INFO mapreduce.Job:  map 30% reduce 0%18/12/12 22:15:45 INFO mapreduce.Job:  map 31% reduce 0%18/12/12 22:15:47 INFO mapreduce.Job:  map 32% reduce 0%18/12/12 22:15:48 INFO mapreduce.Job:  map 33% reduce 4%18/12/12 22:15:51 INFO mapreduce.Job:  map 34% reduce 4%18/12/12 22:15:53 INFO mapreduce.Job:  map 35% reduce 4%18/12/12 22:15:54 INFO mapreduce.Job:  map 36% reduce 6%18/12/12 22:15:55 INFO mapreduce.Job:  map 37% reduce 6%18/12/12 22:15:58 INFO mapreduce.Job:  map 38% reduce 6%18/12/12 22:16:00 INFO mapreduce.Job:  map 39% reduce 8%18/12/12 22:16:03 INFO mapreduce.Job:  map 40% reduce 8%18/12/12 22:16:05 INFO mapreduce.Job:  map 41% reduce 8%18/12/12 22:16:06 INFO mapreduce.Job:  map 41% reduce 10%18/12/12 22:16:08 INFO mapreduce.Job:  map 42% reduce 10%18/12/12 22:16:11 INFO mapreduce.Job:  map 43% reduce 10%18/12/12 22:16:12 INFO mapreduce.Job:  map 44% reduce 10%18/12/12 22:16:15 INFO mapreduce.Job:  map 45% reduce 10%18/12/12 22:16:17 INFO mapreduce.Job:  map 46% reduce 10%18/12/12 22:16:18 INFO mapreduce.Job:  map 46% reduce 11%18/12/12 22:16:19 INFO mapreduce.Job:  map 47% reduce 11%18/12/12 22:16:22 INFO mapreduce.Job:  map 48% reduce 11%18/12/12 22:16:23 INFO mapreduce.Job:  map 49% reduce 11%18/12/12 22:16:24 INFO mapreduce.Job:  map 49% reduce 12%18/12/12 22:16:25 INFO mapreduce.Job:  map 50% reduce 12%18/12/12 22:16:27 INFO mapreduce.Job:  map 51% reduce 12%18/12/12 22:16:29 INFO mapreduce.Job:  map 52% reduce 12%18/12/12 22:16:30 INFO mapreduce.Job:  map 52% reduce 13%18/12/12 22:16:31 INFO mapreduce.Job:  map 53% reduce 13%18/12/12 22:16:35 INFO mapreduce.Job:  map 54% reduce 13%18/12/12 22:16:36 INFO mapreduce.Job:  map 55% reduce 14%18/12/12 22:16:38 INFO mapreduce.Job:  map 56% reduce 14%18/12/12 22:16:42 INFO mapreduce.Job:  map 56% reduce 15%18/12/12 22:16:44 INFO mapreduce.Job:  map 57% reduce 15%18/12/12 22:16:49 INFO mapreduce.Job:  map 57% reduce 16%18/12/12 22:16:50 INFO mapreduce.Job:  map 58% reduce 16%18/12/12 22:16:54 INFO mapreduce.Job:  map 59% reduce 16%18/12/12 22:16:57 INFO mapreduce.Job:  map 60% reduce 16%18/12/12 22:17:00 INFO mapreduce.Job:  map 61% reduce 16%18/12/12 22:17:01 INFO mapreduce.Job:  map 61% reduce 17%18/12/12 22:17:05 INFO mapreduce.Job:  map 62% reduce 17%18/12/12 22:17:09 INFO mapreduce.Job:  map 63% reduce 17%18/12/12 22:17:13 INFO mapreduce.Job:  map 64% reduce 17%18/12/12 22:17:17 INFO mapreduce.Job:  map 65% reduce 17%18/12/12 22:17:20 INFO mapreduce.Job:  map 66% reduce 17%18/12/12 22:17:24 INFO mapreduce.Job:  map 67% reduce 17%18/12/12 22:17:25 INFO mapreduce.Job:  map 67% reduce 18%18/12/12 22:17:27 INFO mapreduce.Job:  map 68% reduce 18%18/12/12 22:17:30 INFO mapreduce.Job:  map 69% reduce 18%18/12/12 22:17:31 INFO mapreduce.Job:  map 70% reduce 18%18/12/12 22:17:34 INFO mapreduce.Job:  map 71% reduce 18%18/12/12 22:17:36 INFO mapreduce.Job:  map 72% reduce 18%18/12/12 22:17:37 INFO mapreduce.Job:  map 73% reduce 19%18/12/12 22:17:39 INFO mapreduce.Job:  map 74% reduce 19%18/12/12 22:17:42 INFO mapreduce.Job:  map 75% reduce 19%18/12/12 22:17:43 INFO mapreduce.Job:  map 75% reduce 20%18/12/12 22:17:44 INFO mapreduce.Job:  map 76% reduce 20%18/12/12 22:17:47 INFO mapreduce.Job:  map 77% reduce 20%18/12/12 22:17:49 INFO mapreduce.Job:  map 78% reduce 20%18/12/12 22:17:50 INFO mapreduce.Job:  map 79% reduce 20%18/12/12 22:17:55 INFO mapreduce.Job:  map 80% reduce 20%18/12/12 22:17:56 INFO mapreduce.Job:  map 81% reduce 20%18/12/12 22:18:00 INFO mapreduce.Job:  map 82% reduce 21%18/12/12 22:18:04 INFO mapreduce.Job:  map 83% reduce 21%18/12/12 22:18:09 INFO mapreduce.Job:  map 84% reduce 21%18/12/12 22:18:16 INFO mapreduce.Job:  map 85% reduce 21%18/12/12 22:18:21 INFO mapreduce.Job:  map 86% reduce 21%18/12/12 22:18:25 INFO mapreduce.Job:  map 86% reduce 22%18/12/12 22:18:29 INFO mapreduce.Job:  map 87% reduce 22%18/12/12 22:18:34 INFO mapreduce.Job:  map 88% reduce 22%18/12/12 22:18:42 INFO mapreduce.Job:  map 89% reduce 22%18/12/12 22:18:46 INFO mapreduce.Job:  map 90% reduce 22%18/12/12 22:18:48 INFO mapreduce.Job:  map 90% reduce 23%18/12/12 22:18:52 INFO mapreduce.Job:  map 91% reduce 23%18/12/12 22:18:55 INFO mapreduce.Job:  map 92% reduce 23%18/12/12 22:19:01 INFO mapreduce.Job:  map 93% reduce 23%18/12/12 22:19:10 INFO mapreduce.Job:  map 94% reduce 23%18/12/12 22:19:12 INFO mapreduce.Job:  map 94% reduce 24%18/12/12 22:19:17 INFO mapreduce.Job:  map 95% reduce 24%18/12/12 22:19:30 INFO mapreduce.Job:  map 96% reduce 24%18/12/12 22:19:38 INFO mapreduce.Job:  map 97% reduce 24%18/12/12 22:19:48 INFO mapreduce.Job:  map 98% reduce 24%18/12/12 22:19:54 INFO mapreduce.Job:  map 98% reduce 25%18/12/12 22:19:57 INFO mapreduce.Job:  map 99% reduce 25%18/12/12 22:20:08 INFO mapreduce.Job:  map 100% reduce 25%18/12/12 22:20:57 INFO mapreduce.Job:  map 100% reduce 26%18/12/12 22:22:58 INFO mapreduce.Job:  map 100% reduce 27%18/12/12 22:24:52 INFO mapreduce.Job:  map 100% reduce 28%18/12/12 22:26:21 INFO mapreduce.Job:  map 100% reduce 29%18/12/12 22:26:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000203_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000203_0 Timed out after 600 secs18/12/12 22:26:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000202_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000202_0 Timed out after 600 secs18/12/12 22:26:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000205_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000205_0 Timed out after 600 secs18/12/12 22:26:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000206_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000206_0 Timed out after 600 secs18/12/12 22:26:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000204_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000204_0 Timed out after 600 secs18/12/12 22:26:28 INFO mapreduce.Job:  map 99% reduce 29%18/12/12 22:26:35 INFO mapreduce.Job:  map 100% reduce 29%18/12/12 22:27:57 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_m_000207_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_m_000207_0 Timed out after 600 secs18/12/12 22:28:04 INFO mapreduce.Job:  map 100% reduce 30%18/12/12 22:28:34 INFO mapreduce.Job:  map 100% reduce 31%18/12/12 22:28:40 INFO mapreduce.Job:  map 100% reduce 33%18/12/12 22:28:46 INFO mapreduce.Job:  map 100% reduce 35%18/12/12 22:28:52 INFO mapreduce.Job:  map 100% reduce 36%18/12/12 22:29:04 INFO mapreduce.Job:  map 100% reduce 37%18/12/12 22:29:16 INFO mapreduce.Job:  map 100% reduce 38%18/12/12 22:29:34 INFO mapreduce.Job:  map 100% reduce 39%18/12/12 22:29:46 INFO mapreduce.Job:  map 100% reduce 40%18/12/12 22:30:16 INFO mapreduce.Job:  map 100% reduce 41%18/12/12 22:31:04 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_r_000008_0, Status : FAILEDError: java.io.IOException: All datanodes [DatanodeInfoWithStorage[10.158.0.22:50010,DS-e05e5f90-91fb-4200-ab51-429c4cbef414,DISK]] are bad. Aborting...\n\tat org.apache.hadoop.hdfs.DataStreamer.handleBadDatanode(DataStreamer.java:1530)\n\tat org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1465)\n\tat org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)\n\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)18/12/12 22:31:05 INFO mapreduce.Job:  map 100% reduce 34%18/12/12 22:31:33 INFO mapreduce.Job:  map 100% reduce 35%18/12/12 22:32:00 INFO mapreduce.Job:  map 100% reduce 36%18/12/12 22:32:18 INFO mapreduce.Job:  map 100% reduce 37%18/12/12 22:34:34 INFO mapreduce.Job:  map 100% reduce 38%18/12/12 22:34:40 INFO mapreduce.Job:  map 100% reduce 40%18/12/12 22:34:46 INFO mapreduce.Job:  map 100% reduce 42%18/12/12 22:34:52 INFO mapreduce.Job:  map 100% reduce 43%18/12/12 22:34:58 INFO mapreduce.Job:  map 100% reduce 44%18/12/12 22:38:12 INFO mapreduce.Job:  map 100% reduce 46%18/12/12 22:38:18 INFO mapreduce.Job:  map 100% reduce 47%18/12/12 22:38:31 INFO mapreduce.Job:  map 100% reduce 48%18/12/12 22:39:19 INFO mapreduce.Job:  map 100% reduce 49%18/12/12 22:39:35 INFO mapreduce.Job:  map 100% reduce 50%18/12/12 22:39:49 INFO mapreduce.Job:  map 100% reduce 51%18/12/12 22:40:13 INFO mapreduce.Job:  map 100% reduce 52%18/12/12 22:40:37 INFO mapreduce.Job:  map 100% reduce 53%18/12/12 22:41:17 INFO mapreduce.Job:  map 100% reduce 54%18/12/12 22:41:23 INFO mapreduce.Job:  map 100% reduce 56%18/12/12 22:41:29 INFO mapreduce.Job:  map 100% reduce 58%18/12/12 22:41:41 INFO mapreduce.Job:  map 100% reduce 59%18/12/12 22:42:10 INFO mapreduce.Job:  map 100% reduce 60%18/12/12 22:42:41 INFO mapreduce.Job:  map 100% reduce 61%18/12/12 22:43:10 INFO mapreduce.Job:  map 100% reduce 62%18/12/12 22:46:29 INFO mapreduce.Job:  map 100% reduce 63%18/12/12 22:50:34 INFO mapreduce.Job:  map 100% reduce 64%18/12/12 22:51:11 INFO mapreduce.Job:  map 100% reduce 65%18/12/12 22:51:29 INFO mapreduce.Job:  map 100% reduce 66%18/12/12 22:51:47 INFO mapreduce.Job:  map 100% reduce 67%18/12/12 22:51:53 INFO mapreduce.Job:  map 100% reduce 68%18/12/12 22:52:00 INFO mapreduce.Job:  map 100% reduce 69%18/12/12 22:52:17 INFO mapreduce.Job:  map 100% reduce 70%18/12/12 22:52:47 INFO mapreduce.Job:  map 100% reduce 71%18/12/12 22:53:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_r_000007_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_r_000007_0 Timed out after 600 secs18/12/12 22:53:28 INFO mapreduce.Job:  map 100% reduce 66%18/12/12 22:53:47 INFO mapreduce.Job:  map 100% reduce 67%18/12/12 22:54:23 INFO mapreduce.Job:  map 100% reduce 68%18/12/12 22:54:24 INFO mapreduce.Job:  map 100% reduce 69%18/12/12 22:54:29 INFO mapreduce.Job:  map 100% reduce 70%18/12/12 22:54:35 INFO mapreduce.Job:  map 100% reduce 71%18/12/12 22:54:36 INFO mapreduce.Job:  map 100% reduce 72%18/12/12 22:54:53 INFO mapreduce.Job:  map 100% reduce 73%18/12/12 22:55:23 INFO mapreduce.Job:  map 100% reduce 74%18/12/12 22:55:53 INFO mapreduce.Job:  map 100% reduce 75%18/12/12 22:56:23 INFO mapreduce.Job:  map 100% reduce 76%18/12/12 22:56:53 INFO mapreduce.Job:  map 100% reduce 77%18/12/12 22:57:24 INFO mapreduce.Job:  map 100% reduce 78%18/12/12 22:57:54 INFO mapreduce.Job:  map 100% reduce 79%18/12/12 22:59:17 INFO mapreduce.Job:  map 100% reduce 80%18/12/12 23:02:23 INFO mapreduce.Job:  map 100% reduce 81%18/12/12 23:04:42 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 23:05:00 INFO mapreduce.Job:  map 100% reduce 83%18/12/12 23:05:18 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 23:05:36 INFO mapreduce.Job:  map 100% reduce 85%18/12/12 23:06:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_r_000002_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_r_000002_0 Timed out after 600 secs18/12/12 23:06:27 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_r_000009_0, Status : FAILEDAttemptID:attempt_1544652597546_0001_r_000009_0 Timed out after 600 secs18/12/12 23:06:28 INFO mapreduce.Job:  map 100% reduce 72%18/12/12 23:07:12 INFO mapreduce.Job:  map 100% reduce 73%18/12/12 23:08:03 INFO mapreduce.Job:  map 100% reduce 74%18/12/12 23:08:59 INFO mapreduce.Job:  map 100% reduce 75%18/12/12 23:10:42 INFO mapreduce.Job:  map 100% reduce 76%18/12/12 23:12:26 INFO mapreduce.Job:  map 100% reduce 77%18/12/12 23:14:12 INFO mapreduce.Job:  map 100% reduce 78%18/12/12 23:15:54 INFO mapreduce.Job:  map 100% reduce 79%18/12/12 23:16:57 INFO mapreduce.Job: Task Id : attempt_1544652597546_0001_r_000003_1, Status : FAILEDAttemptID:attempt_1544652597546_0001_r_000003_1 Timed out after 600 secsContainer killed by the ApplicationMaster.\nSent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 13758 as user hadoop for container container_1544652597546_0001_01_000912, result=success\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 14318/12/12 23:17:42 INFO mapreduce.Job:  map 100% reduce 80%18/12/12 23:19:10 INFO mapreduce.Job:  map 100% reduce 81%18/12/12 23:19:16 INFO mapreduce.Job:  map 100% reduce 82%18/12/12 23:19:22 INFO mapreduce.Job:  map 100% reduce 83%18/12/12 23:19:28 INFO mapreduce.Job:  map 100% reduce 84%18/12/12 23:20:25 INFO mapreduce.Job:  map 100% reduce 85%18/12/12 23:20:30 INFO mapreduce.Job:  map 100% reduce 86%18/12/12 23:20:37 INFO mapreduce.Job:  map 100% reduce 87%18/12/12 23:20:52 INFO mapreduce.Job:  map 100% reduce 88%18/12/12 23:21:34 INFO mapreduce.Job:  map 100% reduce 89%18/12/12 23:23:24 INFO mapreduce.Job:  map 100% reduce 90%18/12/12 23:25:21 INFO mapreduce.Job:  map 100% reduce 91%18/12/12 23:27:19 INFO mapreduce.Job:  map 100% reduce 92%18/12/12 23:29:16 INFO mapreduce.Job:  map 100% reduce 93%18/12/12 23:31:13 INFO mapreduce.Job:  map 100% reduce 94%18/12/12 23:34:51 INFO mapreduce.Job:  map 100% reduce 95%18/12/12 23:35:57 INFO mapreduce.Job:  map 100% reduce 96%18/12/12 23:36:45 INFO mapreduce.Job:  map 100% reduce 97%18/12/12 23:46:03 INFO mapreduce.Job:  map 100% reduce 98%18/12/12 23:49:27 INFO mapreduce.Job:  map 100% reduce 99%18/12/12 23:51:21 INFO mapreduce.Job:  map 100% reduce 100%18/12/13 00:01:38 INFO mapreduce.Job: Job job_1544652597546_0001 completed successfully18/12/13 00:01:39 INFO mapreduce.Job: Counters: 55\n\tFile System Counters\n\t\tFILE: Number of bytes read=382924728850\n\t\tFILE: Number of bytes written=502630923662\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=115000088274\n\t\tHDFS: Number of bytes written=115000000000\n\t\tHDFS: Number of read operations=2655\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=22\n\tJob Counters \n\t\tFailed map tasks=6\n\t\tFailed reduce tasks=5\n\t\tKilled map tasks=8\n\t\tKilled reduce tasks=6\n\t\tLaunched map tasks=887\n\t\tLaunched reduce tasks=22\n\t\tOther local map tasks=8\n\t\tData-local map tasks=803\n\t\tRack-local map tasks=76\n\t\tTotal time spent by all maps in occupied slots (ms)=54451652\n\t\tTotal time spent by all reduces in occupied slots (ms)=65338647\n\t\tTotal time spent by all map tasks (ms)=54451652\n\t\tTotal time spent by all reduce tasks (ms)=65338647\n\t\tTotal vcore-milliseconds taken by all map tasks=54451652\n\t\tTotal vcore-milliseconds taken by all reduce tasks=65338647\n\t\tTotal megabyte-milliseconds taken by all map tasks=55758491648\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=66906774528\n\tMap-Reduce Framework\n\t\tMap input records=1150000000\n\t\tMap output records=1150000000\n\t\tMap output bytes=117300000000\n\t\tMap output materialized bytes=119600057684\n\t\tInput split bytes=88274\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=1150000000\n\t\tReduce shuffle bytes=119600057684\n\t\tReduce input records=1150000000\n\t\tReduce output records=1150000000\n\t\tSpilled Records=4831634628\n\t\tShuffled Maps =9614\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=9614\n\t\tGC time elapsed (ms)=322898\n\t\tCPU time spent (ms)=14575930\n\t\tPhysical memory (bytes) snapshot=263861522432\n\t\tVirtual memory (bytes) snapshot=1782833041408\n\t\tTotal committed heap usage (bytes)=173901611008\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tFile Input Format Counters \n\t\tBytes Read=115000000000\n\tFile Output Format Counters \n\t\tBytes Written=11500000000018/12/13 00:01:39 INFO terasort.TeraSort: donereal\t107m21.976s\nuser\t0m22.802s\nsys\t0m5.296s", :stdout=>"Spent 195ms computing base-splits.Spent 6ms computing TeraScheduler splits.Computing input splits took 202msSampling 10 splits of 874Making 11 from 100000 sampled recordsComputing parititions took 1070msSpent 1275ms computing partitions.", :status=>0}}
Exp  4, overall time taken is 107 m 21.976 s
Exp  4 termintated at 2018-12-13 01:01:39 +0100

Exp  5 started at  2018-12-13 01:14:00 +0100
Exp 5's result is {"10.158.0.1"=>{:stderr=>"18/12/13 00:14:54 INFO terasort.TeraSort: starting18/12/13 00:14:55 INFO input.FileInputFormat: Total input files to process : 2318/12/13 00:14:55 ERROR terasort.TeraSort: Cannot create file/output/_partition.lst. Name node is in safe mode.\nThe reported blocks 898 has reached the threshold 0.9990 of total blocks 898. The number of live datanodes 23 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 14 seconds. NamenodeHostName:node1\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1407)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1395)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2278)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2223)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:728)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:413)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)18/12/13 00:14:55 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN impl.BlockReaderFactory: I/O error constructing remote block reader.\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN hdfs.DFSClient: Failed to connect to /10.158.0.18:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN hdfs.DFSClient: Failed to connect to /10.158.0.8:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN hdfs.DFSClient: Failed to connect to /10.158.0.14:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073748502_7678 after checking nodes = [DatanodeInfoWithStorage[10.158.0.8:50010,DS-14752b0a-bb88-4347-8c63-2dc2ae75e23a,DISK]], ignoredNodes = null18/12/13 00:14:55 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073749104_8280 after checking nodes = [DatanodeInfoWithStorage[10.158.0.18:50010,DS-bac2937d-92e6-4b3b-bf10-15a1393df793,DISK]], ignoredNodes = null18/12/13 00:14:55 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073748502_7678 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.8:50010,DS-14752b0a-bb88-4347-8c63-2dc2ae75e23a,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.8:50010,DS-14752b0a-bb88-4347-8c63-2dc2ae75e23a,DISK]. Will get new block locations from namenode and retry...18/12/13 00:14:55 WARN hdfs.DFSClient: Failed to connect to /10.158.0.6:50010 for block, add to deadNodes and continue. java.io.IOException: The client is stopped\njava.io.IOException: The client is stopped\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1519)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1381)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1345)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy10.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat com.sun.proxy.$Proxy11.getServerDefaults(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:661)\n\tat org.apache.hadoop.hdfs.DFSClient.shouldEncryptData(DFSClient.java:1704)\n\tat org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey(DFSClient.java:1710)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.checkTrustAndSend(SaslDataTransferClient.java:209)\n\tat org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient.peerSend(SaslDataTransferClient.java:160)\n\tat org.apache.hadoop.hdfs.DFSUtilClient.peerFromSocketAndKey(DFSUtilClient.java:568)\n\tat org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:2880)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:815)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:740)\n\tat org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:385)\n\tat org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:696)\n\tat org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655)\n\tat org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:926)\n\tat org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:982)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$TeraRecordReader.nextKeyValue(TeraInputFormat.java:257)\n\tat org.apache.hadoop.examples.terasort.TeraInputFormat$1.run(TeraInputFormat.java:154)18/12/13 00:14:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 720.2920939032217 msec.18/12/13 00:14:55 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073749104_8280 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.18:50010,DS-bac2937d-92e6-4b3b-bf10-15a1393df793,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.18:50010,DS-bac2937d-92e6-4b3b-bf10-15a1393df793,DISK]. Will get new block locations from namenode and retry...18/12/13 00:14:55 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073748517_7693 after checking nodes = [DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK]], ignoredNodes = null18/12/13 00:14:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1949.9928045094923 msec.18/12/13 00:14:55 WARN hdfs.DFSClient: No live nodes contain block BP-1262683191-10.158.0.1-1544648756834:blk_1073748425_7601 after checking nodes = [DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK]], ignoredNodes = null18/12/13 00:14:55 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073748517_7693 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.14:50010,DS-a75f6898-ebaf-46e9-9bf0-617c9eb187ff,DISK]. Will get new block locations from namenode and retry...18/12/13 00:14:55 INFO hdfs.DFSClient: Could not obtain BP-1262683191-10.158.0.1-1544648756834:blk_1073748425_7601 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK] Dead nodes:  DatanodeInfoWithStorage[10.158.0.6:50010,DS-b4ef2379-824f-4fdc-bdad-c8ceb8018a25,DISK]. Will get new block locations from namenode and retry...18/12/13 00:14:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 578.1369373445726 msec.18/12/13 00:14:55 WARN hdfs.DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2846.3422891617674 msec.real\t0m1.876s\nuser\t0m4.467s\nsys\t0m0.406s", :stdout=>"Spent 180ms computing base-splits.Spent 4ms computing TeraScheduler splits.\nComputing input splits took 184msSampling 10 splits of 874", :status=>255}}
Exp  5, overall time taken is 0 m 1.876 s
Exp  5 termintated at 2018-12-13 01:14:55 +0100

